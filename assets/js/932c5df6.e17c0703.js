"use strict";(self.webpackChunkmare_ai=self.webpackChunkmare_ai||[]).push([[1617],{28453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>a});var r=i(96540);const t={},o=r.createContext(t);function s(e){const n=r.useContext(o);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),r.createElement(o.Provider,{value:n},e.children)}},86299:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>a,default:()=>h,frontMatter:()=>s,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"getting-started/inference/onnx","title":"Optimizing models for inference with ONNX","description":"Marie-AI supports the ONNX format for inference. ONNX is an open format for machine learning models that allows you to easily move models between different frameworks.","source":"@site/docs/getting-started/inference/onnx.md","sourceDirName":"getting-started/inference","slug":"/getting-started/inference/onnx","permalink":"/docs/getting-started/inference/onnx","draft":false,"unlisted":false,"editUrl":"https://github.com/marieai/marie-ai/tree/main/docs/docs/getting-started/inference/onnx.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Inference","permalink":"/docs/category/inference"},"next":{"title":"Undervolt NVIDIA GPUs","permalink":"/docs/getting-started/inference/undervolt"}}');var t=i(74848),o=i(28453);const s={sidebar_position:3},a="Optimizing models for inference with ONNX",d={},l=[{value:"TensorRT support",id:"tensorrt-support",level:2},{value:"Setup",id:"setup",level:3},{value:"Optimize Pix2Pix models export for document overlay",id:"optimize-pix2pix-models-export-for-document-overlay",level:2},{value:"Requirement",id:"requirement",level:2},{value:"Install fairseq and requirement",id:"install-fairseq-and-requirement",level:3}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"optimizing-models-for-inference-with-onnx",children:"Optimizing models for inference with ONNX"})}),"\n",(0,t.jsxs)(n.p,{children:["Marie-AI supports the ",(0,t.jsx)(n.a,{href:"https://onnx.ai/",children:"ONNX"})," format for inference. ONNX is an open format for machine learning models that allows you to easily move models between different frameworks."]}),"\n",(0,t.jsx)(n.h1,{id:"exporting-models-to-onnx",children:"Exporting models to ONNX"}),"\n",(0,t.jsx)(n.p,{children:"There are many ways to export models to ONNX. Here we will show you how to export our PyTorch model to ONNX."}),"\n",(0,t.jsx)(n.h2,{id:"tensorrt-support",children:"TensorRT support"}),"\n",(0,t.jsx)(n.p,{children:"TensorRT is a high-performance deep learning inference platform that delivers low latency and high-throughput for deep learning inference applications."}),"\n",(0,t.jsx)(n.h3,{id:"setup",children:"Setup"}),"\n",(0,t.jsx)(n.p,{children:"We need to setup the environment to support TensorRT."}),"\n",(0,t.jsx)(n.h2,{id:"optimize-pix2pix-models-export-for-document-overlay",children:"Optimize Pix2Pix models export for document overlay"}),"\n",(0,t.jsx)(n.p,{children:"In the tools directory we have a script that will export the Pix2Pix model to ONNX."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sh",children:"python3 tools/convert_onnx_overlay.py \\\n    --model_path /mnt/data/marie-ai/model_zoo/pix2pix/pix2pix.pth \\\n    --output_path /mnt/data/marie-ai/model_zoo/pix2pix/pix2pix.onnx\n"})}),"\n",(0,t.jsx)(n.h1,{id:"optimize-fairseq-models-for-ocr",children:"Optimize Fairseq models for OCR"}),"\n",(0,t.jsx)(n.p,{children:"Fairseq is used by Marie-AI for OCR as it is based on the TrOCR model."}),"\n",(0,t.jsxs)(n.p,{children:["We have a custom build of Fairseq that supports ONNX export and inference.\nYou can find the source code ",(0,t.jsx)(n.a,{href:"https://github.com/marieai/fairseq",children:"here"})]}),"\n",(0,t.jsx)(n.p,{children:"To optimize Fairseq workflow we have to do the following:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Divide model into Encoder and Decoder two parts, and separately export to onnx model."}),"\n",(0,t.jsx)(n.li,{children:"Because of the model structure define by input seq_len, should export dynamic shape onnx model."}),"\n",(0,t.jsx)(n.li,{children:"Replace the Fairseq TextRecognitionGenerator task pipeline Encoder and Decoder into ONNX inference model."}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Encoder and Decoder:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Encoder is for extracting feature information from image."}),"\n",(0,t.jsx)(n.li,{children:"Decoder is for decoding the feature information to generate text information."}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"requirement",children:"Requirement"}),"\n",(0,t.jsxs)(n.p,{children:["Checkout the source code and install the package in editable mode.\nReference: ",(0,t.jsx)(n.a,{href:"https://github.com/marieai/fairseq.git",children:"GitHub: Fairseq-MarieAI"})]}),"\n",(0,t.jsx)(n.h3,{id:"install-fairseq-and-requirement",children:"Install fairseq and requirement"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sh",children:"git clone https://github.com/marieai/fairseq.git\ncd fairseq\npip install --editable .\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Edit ",(0,t.jsx)(n.code,{children:"sequence_generator.py"})," to apply changes to the source code."]}),"\n",(0,t.jsx)(n.h1,{id:"references",children:"References"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://onnx.ai/",children:"ONNX"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://developer.nvidia.com/tensorrt",children:"TensorRT"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://blog.openvino.ai/blog-posts/openvino-tm-optimizer-fairseq-s2t-model",children:"openvino"})}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.a,{href:"https://github.com/facebookresearch/fairseq/issues/1669",children:"https://github.com/facebookresearch/fairseq/issues/1669"}),"\n",(0,t.jsx)(n.a,{href:"https://github.com/18582088138/fairseq-openvino/blob/bc61ffe59ae79870815d34d2664a9fffe6d9c694/fairseq/sequence_generator.py",children:"https://github.com/18582088138/fairseq-openvino/blob/bc61ffe59ae79870815d34d2664a9fffe6d9c694/fairseq/sequence_generator.py"})]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}}}]);