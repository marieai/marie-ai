import argparse
import concurrent.futures
import distutils.util
import glob
import hashlib
import io
import json
import logging
import multiprocessing as mp
import os
import random
import shutil
import string
import sys
import time
import uuid
from functools import lru_cache
from multiprocessing import Pool

import cv2
import numpy as np
import rstr
from faker import Faker
from faker.providers import BaseProvider
from PIL import Image, ImageDraw, ImageFont

from marie.boxes import BoxProcessorUlimDit
from marie.boxes.box_processor import PSMode
from marie.boxes.line_processor import find_line_number
from marie.document.trocr_ocr_processor import TrOcrProcessor
from marie.numpyencoder import NumpyEncoder
from marie.utils.overlap import compute_iou, find_overlap_horizontal
from marie.utils.utils import ensure_exists

# FUNSD format can be found here
# https://guillaumejaume.github.io/FUNSD/description/

logger = logging.getLogger(__name__)

# setup data aug
fake = Faker()
fake_names_only = Faker(["it_IT", "en_US", "es_MX", "en_IN"])  # 'de_DE',


# create new provider class


class MemberProvider(BaseProvider):
    def __init__(self, generator):
        super().__init__(generator)
        self.possible_regexes = [
            "^C\d{5}$",
            "^0\d{7}$",
            "^5\d{9}$",
            "^C[0-9]*",
            "^H[0-9]*",
            "\d{6}-\d+",
            "^(\d{7})$",
            "^A\d{14}$",
            "^[0-9]{9}$",
            "^\d{10}WF$",
            "^1N[0-9]+$",
            "^HRP\d{8}$",
            "^(0{0}|0{3})1\d{8}$",
            "^(33)[0-9A-Z]{2,4}$",
            "^P1.+$|^PZ[0-9]{6}$",
            # "^C\d{1}[A-Z0-9]{6}[A-Z]-{2}$",
            "^[0-9]{6}\\-C[0-9]{6}$",
            "^(?!W)[A-Z]{3}[0-9]{5}$",
            "^[0-9]{5,8}-[0-9]{5,6}P",
            "(^100\d{1}\.\d{4}$)|(^\d{4}\/\d{4,6}-\d{4,6}$)|(^\d{5}-\d{4,6}$)|(^1\d{7}$)|(^0\d{7,13}$)|^[0-9]{4,6}[-/\\][0-9]{4,6}$",
            "^(\d{10}(YN))$|^\d{6}[A-Z]{1}\d{3}(YN)$",
        ]

    def member_id(self) -> str:
        # print(rstr.xeger(r'[A-Z]\d[A-Z] \d[A-Z]\d'))
        sel_reg = random.choice(self.possible_regexes)
        val = rstr.xeger(sel_reg)

        # remove all not valid characters
        punctuation = "-._ "
        printable = string.digits + string.ascii_letters + punctuation
        val = "".join(char for char in val if char in printable)

        # There are cases when we will add prexif / suffix to the PAN seperated with space
        if " " not in val and np.random.choice([0, 1], p=[0.7, 0.3]):
            N = random.choice([2, 3, 4])
            res = "".join(random.choices(string.ascii_letters, k=N))
            if np.random.choice([1, 0], p=[0.7, 0.3]):
                val = f"{res} {val}"
            else:
                val = f"{val} {res}"

        val = val.lower()
        if np.random.choice([0, 1], p=[0.5, 0.5]):
            val = val.upper()

        return val


fake.add_provider(MemberProvider)


def from_json_file(filename):
    with io.open(filename, "r", encoding="utf-8") as json_file:
        data = json.load(json_file)
        return data


def __scale_height(img, target_size, method=Image.BILINEAR):
    ow, oh = img.size
    scale = oh / target_size
    w = ow / scale
    h = target_size  # int(max(oh / scale, crop_size))
    resized = img.resize((int(w), int(h)), method)
    return resized, resized.size


def load_image(image_path):
    if not os.path.exists(image_path):
        raise FileNotFoundError(image_path)

    image = cv2.imread(image_path)
    h, w = image.shape[0], image.shape[1]
    return image, (w, h)


def __convert_coco_to_funsd(
    src_dir: str,
    output_path: str,
    annotations_filename: str,
    config: object,
    strip_file_name_path: bool,
) -> None:
    """
    Convert CVAT annotated COCO dataset into FUNSD compatible format for finetuning models.
    """
    print("******* Conversion info ***********")
    print(f"src_dir     : {src_dir}")
    print(f"output_path : {output_path}")
    print(f"annotations : {annotations_filename}")
    print(f"strip_file_name_path : {strip_file_name_path}")

    debug_found_pair = False

    data = from_json_file(annotations_filename)
    categories = data["categories"]
    images = data["images"]
    annotations = data["annotations"]
    images_by_id = {}

    for img in images:
        if strip_file_name_path:
            file_name = img["file_name"]
            img["file_name"] = file_name.split("/")[-1]
        images_by_id[int(img["id"])] = img

    print(f"images_by_id : {len(images_by_id)}")
    cat_id_name = {}
    cat_name_id = {}

    if "question_answer_map" not in config:
        raise Exception(f"Expected key missing : question_answer_map")

    if "id_map" not in config:
        raise Exception(f"Expected key missing : id_map")

    if "link_map" not in config:
        raise Exception(f"Expected key missing : link_map")

    # Expected group mapping that will get translated into specific linking
    # If this validation fails we will stop processing  and report.
    question_answer_map = config["question_answer_map"]
    id_map = config["id_map"]
    link_map = config["link_map"]

    for category in categories:
        cat_id_name[category["id"]] = category["name"]
        cat_name_id[category["name"]] = category["id"]

    ano_groups = {}
    # Group annotations by image_id as their key
    for ano in annotations:
        if ano["image_id"] not in ano_groups:
            ano_groups[ano["image_id"]] = []
        ano_groups[ano["image_id"]].append(ano)

    errors = []
    print("Total images : ", len(images_by_id))
    print("Total groups : ", len(ano_groups))
    continue_on_missing_mapping = True

    for group_id in ano_groups:
        grouping = ano_groups[group_id]
        # Validate that each annotation has associated question/answer pair
        found_cat_id = []
        img_data = images_by_id[group_id]
        file_name = img_data["file_name"]
        filename = file_name.split("/")[-1].split(".")[0]

        category_counts = {}
        for ano in grouping:
            found_cat_id.append(ano["category_id"])
            # validate that we don't have duplicate question/answer mappings, we might change this down the road
            cat_name = cat_id_name[ano["category_id"]]
            category_counts[cat_name] = (
                1 if cat_name not in category_counts else category_counts[cat_name] + 1
            )
            count = category_counts[cat_name]
            if False and count > 1:
                msg = f"Duplicate pair found for image_id[{group_id}] : {cat_name}, {count}, {filename}"
                print(msg)
                errors.append(msg)

        # if we have any missing mapping we will abort and fix the labeling data before continuing
        for question, answer in question_answer_map.items():
            qid = cat_name_id[question]
            aid = cat_name_id[answer]
            # we only have question but no answer
            if qid in found_cat_id and aid not in found_cat_id:
                msg = f"Pair not found for image_id[{group_id}] : {question} [{qid}] MISSING -> {answer} [{aid}]"
                print(msg)
                errors.append(msg)
            else:
                if debug_found_pair:
                    print(f"Pair found : {question} [{qid}] -> {answer} [{aid}]")

        # start conversion
        img_data = images_by_id[group_id]
        file_name = img_data["file_name"]
        filename = file_name.split("/")[-1].split(".")[0]
        src_img_path = os.path.join(src_dir, "images", file_name)
        src_img, size = load_image(src_img_path)

        if len(errors) > 0:
            payload = "\n".join(errors)
            errors = []
            if continue_on_missing_mapping:
                print(f"Missing mapping [{file_name}] \n {payload}")
                continue
            else:
                raise Exception(f"Missing mapping \n {payload}")

        form_dict = {"form": []}

        for ano in grouping:
            category_id = ano["category_id"]
            # Convert form XYWH -> xmin,ymin,xmax,ymax
            bbox = [int(x) for x in ano["bbox"]]
            bbox = [bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]]
            category_name = cat_id_name[category_id]
            label = category_name

            gen_id = random.randint(0, 10000000)
            if category_name in id_map:
                gen_id = id_map[category_name]

            item = {
                "id": gen_id,
                "text": "POPULATE_VIA_ICR",
                "box": bbox,
                "linking": [link_map[category_name]],
                "label": label,
                "line_number": -1,
                "words": [
                    {"text": "POPULATE_VIA_ICR_WORD", "box": [0, 0, 0, 0]},
                ],
            }

            form_dict["form"].append(item)

        os.makedirs(os.path.join(output_path, "annotations_tmp"), exist_ok=True)
        os.makedirs(os.path.join(output_path, "images"), exist_ok=True)

        json_path = os.path.join(output_path, "annotations_tmp", f"{filename}.json")
        dst_img_path = os.path.join(output_path, "images", f"{filename}.png")

        if True:
            with open(json_path, "w") as json_file:
                json.dump(form_dict, json_file, indent=4)

            shutil.copyfile(src_img_path, dst_img_path)


def convert_coco_to_funsd(
    src_dir: str, output_path: str, config: object, strip_file_name_path: bool
) -> None:
    """
    Convert CVAT annotated COCO dataset into FUNSD compatible format for finetuning models.
    """
    # instances_default.json
    items = glob.glob(os.path.join(src_dir, "annotations/*.json"))
    if len(items) == 0:
        raise Exception(f"No annotations to process in : {src_dir}")

    os.makedirs(output_path, exist_ok=True)

    for idx, annotations_filename in enumerate(items):
        try:
            print(f"Processing annotation : {annotations_filename}")
            __convert_coco_to_funsd(
                src_dir, output_path, annotations_filename, config, strip_file_name_path
            )
        except Exception as e:
            raise e


def normalize_bbox(bbox, size):
    return [
        int(1000 * bbox[0] / size[0]),
        int(1000 * bbox[1] / size[1]),
        int(1000 * bbox[2] / size[0]),
        int(1000 * bbox[3] / size[1]),
    ]


def from_json_file(filename):
    with io.open(filename, "r", encoding="utf-8") as json_file:
        data = json.load(json_file)
        return data


def image_to_byte_array(image: Image) -> bytes:
    imgByteArr = io.BytesIO()
    image.save(imgByteArr, format=image.format)
    imgByteArr = imgByteArr.getvalue()
    return imgByteArr


def extract_icr_from_meta(snippet, scr_box_xyxy, ocr_meta_results):
    """
    TODO: Move this to ta separate module
    Extract ICR from the snippet using the OCR meta results
    :param snippet:
    :param scr_box_xyxy:
    :param ocr_meta_results:
    :return:
    """
    results = {
        "words": [],
        "lines": [],
    }

    # scr_box -> x0,y0,x1,y1
    # words -> xywh
    # convert scr_box to xywh
    box_xywh = [
        scr_box_xyxy[0],
        scr_box_xyxy[1],
        scr_box_xyxy[2] - scr_box_xyxy[0],
        scr_box_xyxy[3] - scr_box_xyxy[1],
    ]

    h, w = snippet.shape[0], snippet.shape[1]
    scr_box_xyxy = [int(x) for x in scr_box_xyxy]

    # print("**************************")
    # print(f"scr_box_xyxy : {scr_box_xyxy}   : box_xywh : {box_xywh}")

    # find all the intersections of bounding boxes with the snippet box from ocr_meta
    # there is only one page we are processing so we will only have one entry
    ocr_meta = ocr_meta_results[0]
    words = ocr_meta["words"]
    lines = ocr_meta["lines"]

    found_words = []
    found_boxes = []
    has_found_line = False

    for line in lines:
        bbox = line["bbox"]
        line_number = line["line"]

        bbox_xyxy = [bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]]
        iou = compute_iou(bbox_xyxy, scr_box_xyxy)
        # print(f"iou: {iou}  : box: {box_xywh}  : line : {line}")
        if iou > 0.0:
            # has_found_line = True
            # print(f"iou : {iou}  : box: {box_xywh}  : line : {line}")
            wordids = line["wordids"]
            aligned_words = [w for w in words if w["id"] in wordids]
            word_boxes = [w["box"] for w in aligned_words]
            if False:
                for w in aligned_words:
                    print(f"word : {w}")

            overlaps, indexes, scores = find_overlap_horizontal(box_xywh, word_boxes)
            found_boxes.extend([word_boxes[i] for i in indexes])
            found_words.extend([aligned_words[i] for i in indexes])

    for word in found_words:
        results["words"].append(word)

    # group words by line
    lines = {}
    for word in results["words"]:
        line_number = word["line"]
        if line_number not in lines:
            lines[line_number] = []
        lines[line_number].append(word)

    results["lines"] = []
    for line_number in lines:
        line = {
            "line_number": line_number,
            "text": " ".join([word["text"] for word in lines[line_number]]),
            "words": lines[line_number],
        }
        results["lines"].append(line)
    return found_boxes, results


def extract_icr(image, boxp, icrp):
    if not isinstance(image, np.ndarray):
        raise Exception("Expected image in numpy format")

    msg_bytes = image.tobytes(order="C")
    m = hashlib.sha256()
    m.update(msg_bytes)
    checksum = m.hexdigest()

    print(f"checksum = {checksum}")
    json_file = f"/tmp/marie/{checksum}.json"
    ensure_exists("/tmp/marie")

    if os.path.exists(json_file):
        json_data = from_json_file(json_file)
        print(f"From JSONFILE : {json_file}")
        boxes = json_data["boxes"]
        result = json_data["result"]
        return boxes, result

    key = checksum
    boxes, img_fragments, lines, _, line_bboxes = boxp.extract_bounding_boxes(
        key, "field", image, PSMode.SPARSE
    )

    # we found no boxes, so we will creat only one box and wrap a whole image as that
    if boxes is None or len(boxes) == 0:
        print(f"Empty boxes for : {checksum}")
        if True:
            file_path = os.path.join("/tmp/snippet", f"empty_boxes-{checksum}.png")
            cv2.imwrite(file_path, image)

        h = image.shape[0]
        w = image.shape[1]
        boxes = [[0, 0, w, h]]
        img_fragments = [image]
        lines = [1]

    result, overlay_image = icrp.recognize(
        key, "test", image, boxes, img_fragments, lines
    )

    data = {"boxes": boxes, "result": result}
    with open(json_file, "w") as f:
        json.dump(
            data,
            f,
            sort_keys=True,
            separators=(",", ": "),
            ensure_ascii=False,
            indent=4,
            cls=NumpyEncoder,
        )

    return boxes, result


def decorate_funsd(src_dir: str, debug_fragments=False):
    work_dir_boxes = ensure_exists("/tmp/boxes")
    work_dir_icr = ensure_exists("/tmp/icr")
    output_ann_dir = ensure_exists(
        os.path.expanduser(os.path.join(src_dir, "annotations"))
    )
    ocr_annotations_dir = os.path.expanduser(
        "~/datasets/private/corr-indexer/ocr_annotations"
    )  # TODO : Change this to a config value
    if not os.path.exists(ocr_annotations_dir):
        raise Exception(f"OCR Annotations not found : {ocr_annotations_dir}")

    logger.info("⏳ Decorating examples from = %s", src_dir)
    ann_dir = os.path.expanduser(os.path.join(src_dir, "annotations_tmp"))
    img_dir = os.path.expanduser(os.path.join(src_dir, "images"))

    if True:
        boxp = BoxProcessorUlimDit(
            work_dir=work_dir_boxes,
            models_dir="./model_zoo/unilm/dit/text_detection",
            cuda=True,
        )

        icrp = TrOcrProcessor(work_dir=work_dir_icr, cuda=True)

    for guid, file in enumerate(sorted(os.listdir(ann_dir))):
        print(f"guid = {guid}")
        if guid == -1:
            print("BREAKING")
            break

        file_path = os.path.join(ann_dir, file)
        print("Processing : ", file_path)
        with open(file_path, "r", encoding="utf8") as f:
            data = json.load(f)

        found = False
        requires_one = {"paragraph", "greeting", "question"}

        for i, item in enumerate(data["form"]):
            label = item["label"]
            if label in requires_one:
                found = True
                break

        if not found:
            print(f"Skipping document : {guid} : {file}")
            continue

        image_path = os.path.join(img_dir, file)
        image_path = image_path.replace("json", "png")
        image, size = load_image(image_path)
        ocr_json_path = os.path.join(ocr_annotations_dir, file)

        print(f"Processing : {guid} : {file}")
        if not os.path.exists(ocr_json_path):
            raise Exception(
                f"Skipping document : {guid} : {file}, missing ocr annotations : {ocr_json_path}"
            )

        ocr_meta_results = from_json_file(ocr_json_path)
        ocr_meta = ocr_meta_results[0]
        lines = ocr_meta["meta"]["lines"]
        unique_line_ids = sorted(np.unique(lines))
        line_bboxes = ocr_meta["meta"]["lines_bboxes"]

        if False:
            # line_numbers : line number associated with bounding box
            # lines : raw line boxes that can be used for further processing
            _, _, line_numbers, _, line_bboxes = boxp.extract_bounding_boxes(
                file, "lines", image, PSMode.MULTI_LINE
            )

        for i, item in enumerate(data["form"]):
            debug_fragments = False

            # format : x0,y0,x1,y1
            box = np.array(item["box"]).astype(np.int32)
            x0, y0, x1, y1 = box
            snippet = image[y0:y1, x0:x1, :]
            line_number = find_line_number(line_bboxes, [x0, y0, x1 - x0, y1 - y0])
            # each snippet could be on multiple lines
            # export cropped region
            if debug_fragments:
                file_path = os.path.join("/tmp/snippet", f"{guid}-snippet_{i}.png")
                cv2.imwrite(file_path, snippet)

            boxes, results = extract_icr_from_meta(snippet, box, ocr_meta_results)
            # results.pop("meta", None)

            if (
                results is None
                or len(results) == 0
                or results["lines"] is None
                or len(results["lines"]) == 0
            ):
                print(f"No results for : {guid}-{i}")
                continue

            if debug_fragments:
                file_path = os.path.join("/tmp/snippet", f"{guid}-snippet_{i}.png")
                cv2.imwrite(file_path, snippet)

            words = []
            text = " ".join([line["text"] for line in results["lines"]])

            # boxes are in stored in x0,y0,x1,y1 where x0,y0 is upper left corner and x1,y1 if bottom/right
            # we need to account for offset from the snippet box
            # results["word"] are in a xywh format in local position and need to be converted to relative position
            for word in results["words"]:
                w_text = word["text"]
                x, y, w, h = word["box"]
                word_box = [x, y, x + w, y + h]
                adj_word = {"text": w_text, "box": word_box}
                words.append(adj_word)

            item["words"] = words
            item["text"] = text
            item["line_number"] = line_number

        # create masked image for OTHER label
        image_masked, _ = load_image(image_path)
        index = 0

        for i, item in enumerate(data["form"]):
            # format : x0,y0,x1,y1
            box = np.array(item["box"]).astype(np.int32)
            x0, y0, x1, y1 = box
            cv2.rectangle(
                image_masked, (x0, y0), (x1, y1), (255, 255, 255), thickness=-1
            )
            index = i + 1

        debug_fragments = True
        if debug_fragments:
            file_path = os.path.join("/tmp/snippet", f"{guid}-masked.png")
            cv2.imwrite(file_path, image_masked)

        # masked boxes will be same as the original ones
        boxes_masked, results_masked = extract_icr(image_masked, boxp, icrp)

        x0 = 0
        y0 = 0

        print("-------- MASKED ----------")
        current_max_index = data["form"][-1]["id"]
        # Add masked boxes to the end of the list of annotations, with the same line number as the original box
        # some of the boxes may be empty, so we will filter them out
        if results_masked is not None and len(results_masked["words"]) > 0:
            for i, word in enumerate(results_masked["words"]):
                x, y, w, h = word["box"]

                # check if the word is same as image size if so skip it
                if w == image_masked.shape[1] and h == image_masked.shape[0]:
                    print(f"Skipping word due to masked image constrain : {word}")
                    continue

                line_number = find_line_number(line_bboxes, [x, y, w, h])
                word_box = [x, y, x + w, y + h]

                item = {
                    "id": current_max_index + i,
                    "text": word["text"],
                    "box": word_box,
                    "line_number": line_number,
                    "linking": [],  # TODO: Not in use.
                    "label": "other",
                    "words": [{"text": word["text"], "box": word_box}],
                }

                data["form"].append(item)

        # remove all the text nodes that are placeholders for ICR "text": "POPULATE_VIA_ICR"
        data["form"] = [
            item for item in data["form"] if item["text"] != "POPULATE_VIA_ICR"
        ]

        # Find all annotations by line number
        items_by_line = {}
        form_data = []
        for item in data["form"]:
            if "line_number" not in item:
                print(f"skipping item = {item}")
                continue
            if item["line_number"] not in items_by_line:
                items_by_line[item["line_number"]] = []
            items_by_line[item["line_number"]].append(item)
            form_data.append(item)

        data["form"] = form_data
        # Order by line number
        unique_line_numbers = list(items_by_line.keys())
        unique_line_numbers.sort()
        items_by_line = {
            line: np.array(items_by_line[line]) for line in unique_line_numbers
        }

        word_index = 0
        data_form_sorted = []
        # Order annotations by X value (left to right) per line
        for line_number, items_on_line in items_by_line.items():
            boxes_on_line = np.array([item["box"] for item in items_on_line])
            items_on_line = items_on_line[np.argsort(boxes_on_line[:, 0])]

            for item in items_on_line:
                item["word_index"] = word_index
                data_form_sorted.append(item)
                word_index += 1

        data["form"] = data_form_sorted
        json_path = os.path.join(output_ann_dir, file)
        with open(json_path, "w") as json_file:
            json.dump(
                data,
                json_file,
                sort_keys=False,
                separators=(",", ": "),
                ensure_ascii=False,
                indent=2,
                cls=NumpyEncoder,
            )


def generate_pan(num_char):
    import string

    letters = string.ascii_letters.lower()
    if np.random.choice([0, 1], p=[0.5, 0.5]):
        letters = string.digits
    if np.random.choice([0, 1], p=[0.5, 0.5]):
        letters = letters.upper()
    prefix = "".join(random.choice(letters) for i in range(num_char))
    return prefix


@lru_cache(maxsize=20)
def get_cached_font(font_path, font_size):
    # return ImageFont.truetype(font_path, font_size, layout_engine=ImageFont.Layout.BASIC)
    # print(f"Loading font : {font_path}")
    return ImageFont.truetype(font_path, font_size)


def generate_text(label, width, height, font_path):
    """generate text for specific label"""

    avg_line_height = 40
    est_line_count = max(1, height // avg_line_height)
    height = min(height, 50)
    font_size = int(height * 1)

    # Generate text inside image
    font = get_cached_font(font_path, font_size)
    img = Image.new("RGB", (width, height), color=(255, 255, 255))
    draw = ImageDraw.Draw(img)
    # space_w, _ = draw.textsize(" ", font=font)
    space_w, _ = draw.textbbox((0, 0), " ", font=font)[2:4]

    dec = 1
    index = 0
    label_text = ""

    # ADD Generation for following
    # member_number_answer
    # pan_answer
    # member_name_answer
    # patient_name_answer
    # dos_answer
    # check_amt_answer
    # paid_amt_answer
    # billed_amt_answer
    # birthdate_answer
    # check_number_answer
    # claim_number_answer
    # letter_date
    # phone X
    # url X

    while True:
        if index > 5:
            font_size = font_size - dec
            # print(f"Reducing font size : {font_size}")
            font = get_cached_font(font_path, font_size)
            if font_size < 10:
                break
            index = 0
            # space_w, _ = draw.textsize(" ", font=font)
            space_w, _ = draw.textbbox((0, 0), " ", font=font)[2:4]

        if (
            label == "dos_answer"
            or label == "birthdate_answer"
            or label == "letter_date"
            or label == "date"
        ):
            # https://datatest.readthedocs.io/en/stable/how-to/date-time-str.html
            patterns = [
                "%Y%m%d",
                "%Y-%m-%d",
                "%Y/%m/%d",
                "%d/%m/%Y",
                "%m/%d/%Y",
                "%d.%m.%Y",
                "%d %B %Y",
                "%b %d, %Y",
            ]

            # make composite DOS
            # date-date
            # date thought date
            # date to date

            if label == "dos_answer":
                if np.random.choice([0, 1], p=[0.3, 0.7]):
                    pattern = random.choice(patterns)
                    sel_reg = random.choice(["-", " - ", " ", " to ", " thought "])
                    d1 = fake.date(pattern=pattern)
                    d2 = fake.date(pattern=pattern)
                    label_text = f"{d1}{sel_reg}{d2}"
                else:
                    label_text = fake.date(pattern=random.choice(patterns))
            elif (
                label == "birthdate_answer" or label == "letter_date" or label == "date"
            ):
                label_text = fake.date(pattern=random.choice(patterns))

        if label == "pan_answer":
            label_text = fake.member_id()
        if label == "member_number_answer":
            label_text = fake.member_id()
        if label == "claim_number_answer":
            label_text = fake.member_id()

        if (
            label == "member_name_answer"
            or label == "patient_name_answer"
            or label == "provider_answer"
        ):
            label_text = fake_names_only.name()
            if np.random.choice([0, 1], p=[0.5, 0.5]):
                label_text = label_text.upper()

        if label == "phone":
            label_text = fake_names_only.phone_number()

        if label == "identifier":
            N = random.choice([4, 6, 8, 10, 12])
            if np.random.choice([0, 1], p=[0.5, 0.5]):
                label_text = "".join(random.choices(string.digits, k=N))
            else:
                label_text = "".join(random.choices(string.ascii_letters, k=N))
            label_text = label_text.upper()

        if label == "url":
            label_text = fake.domain_name()
            if np.random.choice([0, 1], p=[0.5, 0.5]):
                label_text = fake.company_email()

        if (
            label == "check_amt_answer"
            or label == "paid_amt_answer"
            or label == "billed_amt_answer"
            or label == "money"
        ):
            label_text = fake.pricetag()
            if np.random.choice([0, 1], p=[0.5, 0.5]):
                label_text = label_text.replace("$", "")

        if label == "address":
            if est_line_count == 1:
                label_text = fake.address().replace("\n", " ")
            elif est_line_count == 2:
                label_text = fake.address()
            else:
                label_text = f"{fake.company()}\n{fake.address()}"

        lines = label_text.split("\n")
        line_segments = []
        line_heights = [0 for _ in lines]
        text_width = 0

        for k, local_text in enumerate(lines):
            segments = []
            # partition data into boxes splitting on blank spaces
            text_chunks = local_text.split(" ")
            # _text_width, text_height = draw.textsize(local_text, font=font)
            _text_width, text_height = draw.textbbox((0, 0), local_text, font=font)[2:4]

            line_heights[k] = text_height

            if _text_width > text_width:
                text_width = _text_width

            start_x = 0
            padding_x = space_w // 2

            if len(text_chunks) == 1:
                box = [start_x, 0, width, text_height]
                segments.append({"text": local_text, "box": box})
            else:
                for i, chunk in enumerate(text_chunks):
                    # chunk_width, chunk_height = draw.textsize(chunk, font=font)
                    chunk_width, chunk_height = draw.textbbox((0, 0), chunk, font=font)[
                        2:4
                    ]
                    # x0, y0, x1, y1
                    end_x = min(start_x + chunk_width + padding_x, width)
                    box = [start_x, 0, end_x, text_height]
                    segments.append({"text": chunk, "box": box})
                    start_x += chunk_width
                    if i < len(text_chunks):
                        start_x += space_w

            line_segments.append(segments)
        if text_width < width:
            # print(
            #     f"GEN [{label}, {font_size}, {est_line_count} : {text_height} :  {round(rat, 2)}] : {width} , {height} :  [{text_width}, {text_height} ] >   {label_text}"
            # )
            break
        index = index + 1

    return font_size, label_text, line_segments, line_heights


# @Timer(text="Aug in {:.4f} seconds")
def __augment_decorated_process(
    guid: int, count: int, file_path: str, src_dir: str, dest_dir: str
):
    # Faker.seed(0)
    output_aug_images_dir = ensure_exists(os.path.join(dest_dir, "images"))
    output_aug_annotations_dir = ensure_exists(os.path.join(dest_dir, "annotations"))

    ann_dir = os.path.join(src_dir, "annotations")
    img_dir = os.path.join(src_dir, "images")

    # file_path = os.path.join(ann_dir, file)
    file = file_path.split("/")[-1]
    print(f"File: {file_path}")

    try:
        with open(file_path, "r", encoding="utf8") as f:
            data = json.load(f)
    except Exception as e:
        raise e

    image_path = os.path.join(img_dir, file)
    image_path = image_path.replace("json", "png")
    filename = image_path.split("/")[-1].split(".")[0]

    for k in range(0, count):
        print(f"Iter : {guid} , {k} of {count} ; {filename} ")
        font_face = np.random.choice(
            [
                "FreeSansOblique.ttf",
                # "FreeSansBold.ttf",
                "FreeSans.ttf",
                "OpenSans-Light.ttf",
                "FreeMono.ttf",
                "vpscourt.ttf",
            ]
        )
        font_path = os.path.join("./assets/fonts", font_face)

        data_copy = dict()
        data_copy["form"] = []

        masked_fields = [
            "member_number_answer",
            "pan_answer",
            "member_name_answer",
            "patient_name_answer",
            "dos_answer",
            "check_amt_answer",
            "paid_amt_answer",
            "billed_amt_answer",
            "birthdate_answer",
            "check_number_answer",
            "claim_number_answer",
            "letter_date",
            "phone",
            "url",
            "date",
            "money",
            "provider_answer",
            "identifier",
            "address",
        ]

        image_masked, size = load_image_pil(image_path)
        draw = ImageDraw.Draw(image_masked)

        for i, item in enumerate(data["form"]):
            label = item["label"]
            if label == "other" or label not in masked_fields:
                data_copy["form"].append(item)
                continue

            # pan_answer  dos_answer member_number_answer
            # format : x0,y0,x1,y1
            box = np.array(item["box"]).astype(np.int32)
            x0, y0, x1, y1 = box
            w = x1 - x0
            h = y1 - y0
            xoffset = 5
            yoffset = 0

            # Generate text inside image
            font_size, label_text, segments_lines, line_heights = generate_text(
                label, w, h, font_path
            )

            assert len(line_heights) != 0
            font = get_cached_font(font_path, font_size)

            # x0, y0, x1, y1 = xy
            # Yellow with outline for debug
            # draw.rectangle(
            #     ((x0, y0), (x1, y1)), fill="#FFFFCC", outline="#FF0000", width=1
            # )

            # clear region
            draw.rectangle(((x0, y0), (x1, y1)), fill="#FFFFFF")

            dup_item = item  # copy.copy(item)
            dup_item["text"] = label_text
            dup_item["id"] = str(uuid.uuid4())  # random.randint(50000, 250000)
            dup_item["words"] = []
            dup_item["linking"] = []
            words = []

            total_text_height = 0
            for th in line_heights:
                total_text_height += th

            space = h - total_text_height
            line_offset = 0
            baseline_spacing = max(4, space // len(line_heights))

            for line_idx, segments in enumerate(segments_lines):
                for seg in segments:
                    seg_text = seg["text"]
                    sx0, sy0, sx1, sy1 = seg["box"]
                    sw = sx1 - sx0
                    sh = sy1 - sy0
                    adj_box = [
                        x0 + sx0,
                        y0 + line_offset,
                        x0 + sx0 + sw,
                        y0 + sh + line_offset,
                    ]
                    word = {"text": seg_text, "box": adj_box}
                    words.append(word)
                    # debug box
                    # draw.rectangle(
                    #     ((adj_box[0], adj_box[1]), (adj_box[2], adj_box[3])),
                    #     outline="#FF0000",
                    #     width=1,
                    # )
                line_offset += line_heights[line_idx] + baseline_spacing

            dup_item["words"] = words

            line_offset = 0

            for line_idx, text_line in enumerate(label_text.split("\n")):
                draw.text(
                    (x0 + xoffset, y0 + line_offset),
                    text=text_line,
                    fill="#000000",
                    font=font,
                    stroke_fill=1,
                )
                line_offset += line_heights[line_idx] + baseline_spacing
            data_copy["form"].append(dup_item)

        # Save items
        out_name_prefix = f"{filename}_{guid}_{k}"

        json_path = os.path.join(output_aug_annotations_dir, f"{out_name_prefix}.json")
        dst_img_path = os.path.join(output_aug_images_dir, f"{out_name_prefix}.png")

        # print(f'Writing : {json_path}')
        with open(json_path, "w") as json_file:
            json.dump(
                data_copy,
                json_file,
                # sort_keys=True,
                separators=(",", ": "),
                ensure_ascii=False,
                indent=2,
                cls=NumpyEncoder,
            )

        # saving in JPG format as it is substantially faster than PNG
        # image_masked.save(
        #     os.path.join("/tmp/snippet", f"{out_name_prefix}.jpg"), quality=100
        # )  # 100 disables compression
        #
        # image_masked.save(os.path.join("/tmp/snippet", f"{out_name_prefix}.png"), compress_level=1)
        image_masked.save(dst_img_path, compress_level=2)

        del draw


def augment_decorated_annotation(count: int, src_dir: str, dest_dir: str):
    ann_dir = os.path.join(src_dir, "annotations")
    # mp.cpu_count()

    if False:
        for guid, file in enumerate(sorted(os.listdir(ann_dir))):
            file_path = os.path.join(ann_dir, file)
            __augment_decorated_process(guid, count, file_path, src_dir, dest_dir)

    # slower comparing to  pool.starmap
    if False:
        futures = []
        with concurrent.futures.ThreadPoolExecutor(
            max_workers=int(mp.cpu_count() * 0.75)
        ) as executor:
            for guid, file in enumerate(sorted(os.listdir(ann_dir))):
                file_path = os.path.join(ann_dir, file)
                feature = executor.submit(
                    __augment_decorated_process,
                    guid,
                    count,
                    file_path,
                    src_dir,
                    dest_dir,
                )
                futures.append(feature)

        for future in concurrent.futures.as_completed(futures):
            try:
                print(future.result())
            except Exception as e:
                print(e)

            print("All tasks has been finished")

    if True:
        args = []
        for guid, file in enumerate(sorted(os.listdir(ann_dir))):
            file_path = os.path.join(ann_dir, file)
            __args = (guid, count, file_path, src_dir, dest_dir)
            args.append(__args)

        results = []
        start = time.time()
        print("\nPool Executor:")
        print("Time elapsed: %s" % (time.time() - start))

        pool = Pool(processes=int(mp.cpu_count() * 0.95))
        pool_results = pool.starmap(__augment_decorated_process, args)

        pool.close()
        pool.join()

        print("Time elapsed[submitted]: %s" % (time.time() - start))
        for r in pool_results:
            print("Time elapsed[result]: %s  , %s" % (time.time() - start, r))
            # results.append(result)
        print("Time elapsed[all]: %s" % (time.time() - start))


def load_image_pil(image_path):
    image = Image.open(image_path).convert("RGB")
    w, h = image.size
    return image, (w, h)


def visualize_funsd(src_dir: str, dst_dir: str, config: dict):
    ann_dir = os.path.join(src_dir, "annotations")
    img_dir = os.path.join(src_dir, "images")

    os.makedirs(dst_dir, exist_ok=True)

    for guid, file in enumerate(sorted(os.listdir(ann_dir))):
        file_path = os.path.join(ann_dir, file)

        print(f"file_path : {file_path}")
        with open(file_path, "r", encoding="utf8") as f:
            data = json.load(f)

        filename = file.split("/")[-1].split(".")[0]
        image_path = os.path.join(img_dir, file)
        image_path = image_path.replace("json", "png")
        print(f"image_path : {image_path}")
        print(f"filename : {filename}")
        image, size = load_image_pil(image_path)

        # draw predictions over the image
        draw = ImageDraw.Draw(image, "RGBA")
        font = ImageFont.load_default()
        # https://stackoverflow.com/questions/54165439/what-are-the-exact-color-names-available-in-pils-imagedraw
        label2color = config["label2color"]

        for i, item in enumerate(data["form"]):
            predicted_label = item["label"].lower()
            color = label2color[predicted_label]
            box_xyxy = item["box"]
            box_xywh = [
                box_xyxy[0],
                box_xyxy[1],
                box_xyxy[2] - box_xyxy[0],
                box_xyxy[3] - box_xyxy[1],
            ]

            # print("---------------------------------")
            # print(f"predicted_label : {predicted_label}")
            # print(f"  **  : {box_xyxy}  : {box_xywh}")
            # print(item["text"])

            #  (x0, y0, x1, y1)
            for word in item["words"]:
                box_xyxy = word["box"]
                box_xywh = [
                    box_xyxy[0],
                    box_xyxy[1],
                    box_xyxy[2] - box_xyxy[0],
                    box_xyxy[3] - box_xyxy[1],
                ]
                # print(f"  >> {word['text']} : {box_xyxy}  : {box_xywh}")
                # draw.rectangle(box_xyxy, outline=color, width=1)

            box = item["box"]
            if predicted_label != "other":
                draw.rectangle(
                    box,
                    outline=color,
                    width=1,
                )  # fill=(0, 180, 0, 50)
            else:
                draw.rectangle(box, outline=color, width=1)

            predicted_label = f"{i} - {predicted_label}"
            draw.text(
                (box[0] + 10, box[1] - 10),
                text=predicted_label,
                fill=color,
                font=font,
                stroke_width=0,
            )

        image.save(os.path.join(dst_dir, f"viz_{filename}.png"))


@lru_cache(maxsize=10)
def resize_align_bbox(bbox, orig_w, orig_h, target_w, target_h):
    clip_to_y = 1000

    x_scale = target_w / orig_w
    y_scale = target_h / orig_h
    orig_left, orig_top, orig_right, orig_bottom = bbox
    x = int(np.round(orig_left * x_scale))
    y = int(np.round(orig_top * y_scale))
    xmax = int(np.round(orig_right * x_scale))
    ymax = int(np.round(orig_bottom * y_scale))
    return [x, y, xmax, min(ymax, clip_to_y)]


def rescale_annotation_frame(src_json_path: str, src_image_path: str):
    print(
        f"Recalling annotation : {src_json_path.split('/')[-1]}, {src_image_path.split('/')[-1]}"
    )

    filename = src_image_path.split("/")[-1].split(".")[0]
    image, orig_size = load_image_pil(src_image_path)
    resized, target_size = __scale_height(image, 1000)
    # resized.save(f"/tmp/snippet/resized_{filename}.png")

    # print(f"orig_size, target_size   = {orig_size} : {target_size}")
    orig_w, orig_h = orig_size
    target_w, target_h = target_size
    data = from_json_file(src_json_path)

    try:
        for i, item in enumerate(data["form"]):
            bbox = tuple(item["box"])  # np.array(item["box"]).astype(np.int32)
            item["box"] = resize_align_bbox(bbox, orig_w, orig_h, target_w, target_h)
            for word in item["words"]:
                bbox = tuple(word["box"])  # np.array(item["box"]).astype(np.int32)
                word["box"] = resize_align_bbox(
                    bbox, orig_w, orig_h, target_w, target_h
                )
    except Exception as ex:
        print(src_json_path)
        print(ex)
        raise ex

    return data, resized


def __rescale_annotate_frames(
    ann_dir_dest, img_dir_dest, filename, json_path, image_path
):
    # if False and filename != "152618378_2":
    #     return

    # 152630220_3  152618378_2 152618400  152624795_3
    print(f"filename : {filename}")
    print(f"json_path : {json_path}")
    print(f"image_path : {image_path}")

    data, image = rescale_annotation_frame(json_path, image_path)

    # Figure out how to handle this
    # if the width > 1000 SKIP for now
    if max(image.size) > 1000:
        print(f"Skipping image due to size[{image.size}] : {filename}")
        return

    json_path_dest = os.path.join(ann_dir_dest, f"{filename}.json")
    image_path_dest = os.path.join(img_dir_dest, f"{filename}.png")
    # save image and json data
    image.save(image_path_dest)

    with open(json_path_dest, "w") as json_file:
        json.dump(
            data,
            json_file,
            sort_keys=True,
            separators=(",", ": "),
            ensure_ascii=False,
            indent=2,
            cls=NumpyEncoder,
        )


def rescale_annotate_frames(src_dir: str, dest_dir: str):
    if False:
        print("Skipping rescale_annotate_frames")
        return

    ann_dir = os.path.join(src_dir, "annotations")
    img_dir = os.path.join(src_dir, "images")

    ann_dir_dest = ensure_exists(os.path.join(dest_dir, "annotations"))
    img_dir_dest = ensure_exists(os.path.join(dest_dir, "images"))

    if False:
        for guid, file in enumerate(sorted(os.listdir(ann_dir))):
            json_path = os.path.join(ann_dir, file)
            filename = file.split("/")[-1].split(".")[0]
            image_path = os.path.join(img_dir, file).replace("json", "png")
            __rescale_annotate_frames(
                ann_dir_dest, img_dir_dest, filename, json_path, image_path
            )

    if True:
        args = []
        for guid, file in enumerate(sorted(os.listdir(ann_dir))):
            json_path = os.path.join(ann_dir, file)
            filename = file.split("/")[-1].split(".")[0]
            image_path = os.path.join(img_dir, file).replace("json", "png")
            __args = (ann_dir_dest, img_dir_dest, filename, json_path, image_path)
            args.append(__args)

        results = []
        start = time.time()
        print("\nPool Executor:")
        print("Time elapsed: %s" % (time.time() - start))

        pool = Pool(processes=mp.cpu_count())
        # pool = Pool(processes=1)
        pool_results = pool.starmap(__rescale_annotate_frames, args)

        pool.close()
        pool.join()

        print("Time elapsed[submitted]: %s" % (time.time() - start))
        for r in pool_results:
            print("Time elapsed[result]: %s  , %s" % (time.time() - start, r))
        print("Time elapsed[all]: %s" % (time.time() - start))


def split_dataset(src_dir, output_path, split_percentage):
    """
    Split CODO dataset for training and test.

    ARGS:
        src_dir  : input folder for COCO dataset, expected structure to have 'annotations' and 'image' folder
        output_path : output folder where new train/test directory will be created
        split_percentage      : split ration ex: .8  this will create 80/20 Train/Test split
    """

    print("Split dataset")
    print("src_dir     = {}".format(src_dir))
    print("output_path = {}".format(output_path))
    print("split       = {}".format(split_percentage))

    ann_dir = os.path.join(src_dir, "annotations")
    img_dir = os.path.join(src_dir, "images")

    if not os.path.exists(ann_dir):
        raise Exception("Source directory missing expected 'annotations' sub-directory")

    if not os.path.exists(img_dir):
        raise Exception("Source directory missing expected 'images' sub-directory")

    # shuffle the dataset
    ann_files = os.listdir(ann_dir)
    random.shuffle(ann_files)
    file_set = []

    for guid, file in enumerate(ann_files):
        json_path = os.path.join(ann_dir, file)
        image_path = os.path.join(img_dir, file).replace("json", "png")
        filename = image_path.split("/")[-1].split(".")[0]
        item = {"annotation": json_path, "image": image_path, "filename": filename}
        file_set.append(item)

    total_count = len(file_set)
    sample_count = int(total_count * split_percentage)
    print(f"split_percentage = {split_percentage}")
    print(f"total_count      = {total_count}")
    print(f"sample_count     = {sample_count}")
    print(f"output_path      = {output_path}")

    ann_dir_out_train = os.path.join(output_path, "train", "annotations")
    img_dir_out_train = os.path.join(output_path, "train", "images")

    ann_dir_out_test = os.path.join(output_path, "test", "annotations")
    img_dir_out_test = os.path.join(output_path, "test", "images")

    if os.path.exists(os.path.join(output_path, "train")) or os.path.exists(
        os.path.join(output_path, "test")
    ):
        raise Exception(
            "Output directory not empty, manually remove test/train directories."
        )

    os.makedirs(ann_dir_out_train, exist_ok=True)
    os.makedirs(img_dir_out_train, exist_ok=True)
    os.makedirs(ann_dir_out_test, exist_ok=True)
    os.makedirs(img_dir_out_test, exist_ok=True)

    train_set = file_set[0:sample_count]
    test_set = file_set[sample_count:-1]

    print(f"Train size : {len(train_set)}")
    print(f"Test size : {len(test_set)}")

    splits = [
        {
            "name": "train",
            "files": train_set,
            "ann_dir_out": ann_dir_out_train,
            "img_dir_out": img_dir_out_train,
        },
        {
            "name": "test",
            "files": test_set,
            "ann_dir_out": ann_dir_out_test,
            "img_dir_out": img_dir_out_test,
        },
    ]

    for split in splits:
        fileset = split["files"]
        ann_dir_out = split["ann_dir_out"]
        img_dir_out = split["img_dir_out"]

        for item in fileset:
            ann = item["annotation"]
            img = item["image"]
            filename = item["filename"]
            shutil.copyfile(ann, os.path.join(ann_dir_out, f"{filename}.json"))
            shutil.copyfile(img, os.path.join(img_dir_out, f"{filename}.png"))


def default_decorate(args: object):
    print("Default decorate")
    print(args)
    print("*" * 180)

    # This should be our dataset folder
    mode = args.mode
    src_dir = os.path.join(args.dir, f"{mode}")
    decorate_funsd(src_dir, debug_fragments=False)


def default_augment(args: object):
    print("Default augment")
    print(args)
    print("*" * 180)
    # This should be our dataset folder
    mode = args.mode
    aug_count = args.count
    args.dir = os.path.expanduser(args.dir)
    root_dir = args.dir
    src_dir = os.path.join(args.dir, f"{mode}")
    dst_dir = (
        args.dir_output
        if args.dir_output != "./augmented"
        else os.path.abspath(os.path.join(root_dir, f"{mode}-augmented"))
    )

    print(f"mode      = {mode}")
    print(f"aug_count = {aug_count}")
    print(f"src_dir   = {src_dir}")
    print(f"dst_dir   = {dst_dir}")

    augment_decorated_annotation(count=aug_count, src_dir=src_dir, dest_dir=dst_dir)


def default_rescale(args: object):
    print("Default rescale")
    print(args)
    print("*" * 180)

    # This should be our dataset folder
    mode = args.mode
    suffix = args.suffix
    root_dir = args.dir
    src_dir = os.path.join(args.dir, f"{mode}{suffix}")

    dst_dir = (
        args.dir_output
        if args.dir_output != "./rescaled"
        else os.path.abspath(os.path.join(root_dir, f"{mode}-rescaled"))
    )

    print(f"mode    = {mode}")
    print(f"suffix  = {suffix}")
    print(f"src_dir = {src_dir}")
    print(f"dst_dir = {dst_dir}")

    rescale_annotate_frames(src_dir=src_dir, dest_dir=dst_dir)


def default_visualize(args: object):
    print("Default visualize")
    print(args)
    print("*" * 180)

    src_dir = os.path.expanduser(args.dir)
    dst_dir = os.path.expanduser(args.dir_output)
    args.config = os.path.expanduser(args.config)

    print(f"src_dir   = {src_dir}")
    print(f"dst_dir   = {dst_dir}")
    print(f"config    = {args.config}")
    # load config file
    config = from_json_file(args.config)

    visualize_funsd(src_dir, dst_dir, config)


def default_convert(args: object):
    print("Default convert")
    print(args)
    print("*" * 180)
    mode = args.mode
    args.dir = os.path.expanduser(args.dir)
    suffix = args.mode_suffix
    strip_file_name_path = args.strip_file_name_path
    src_dir = os.path.join(args.dir, f"{mode}{suffix}")
    args.config = os.path.expanduser(args.config)

    dst_path = (
        args.dir_converted
        if args.dir_converted != "./converted"
        else os.path.join(args.dir, "output", "dataset", f"{mode}")
    )

    if not os.path.exists(args.config):
        raise FileNotFoundError(f"File not found : {args.config}")

    # load config file
    config = from_json_file(args.config)

    print(f"mode       = {mode}")
    print(f"suffix     = {suffix}")
    print(f"src_dir    = {src_dir}")
    print(f"dst_path   = {dst_path}")

    convert_coco_to_funsd(src_dir, dst_path, config, strip_file_name_path)


def default_all_steps(args: object):
    from argparse import Namespace

    print("Default all_steps")
    print(args)
    print("*" * 180)

    root_dir = args.dir
    aug_count = args.aug_count
    dataset_dir = os.path.join(root_dir, "output", "dataset")

    # clone and remove  unused values
    args_1 = vars(args).copy()
    args_1["func"] = None
    args_1["command"] = "convert"
    args_1["dir_converted"] = "./converted"

    args_2 = vars(args).copy()
    args_2["func"] = None
    args_2["command"] = "decorate"
    args_2["dir"] = dataset_dir

    args_3 = vars(args).copy()
    args_3["func"] = None
    args_3["command"] = "augment"
    args_3["dir"] = dataset_dir
    args_3["count"] = aug_count
    args_3["dir_output"] = "./augmented"

    args_4 = vars(args).copy()
    args_4["func"] = None
    args_4["command"] = "rescale"
    args_4["dir"] = dataset_dir
    args_4["dir_output"] = "./rescaled"
    args_4["suffix"] = "-augmented"

    # execute each step
    default_convert(Namespace(**args_1))
    default_decorate(Namespace(**args_2))
    default_augment(Namespace(**args_3))
    default_rescale(Namespace(**args_4))


def default_split(args: object):
    print("Default split")
    print(args)
    print("*" * 180)

    src_dir = args.dir
    dst_dir = args.dir_output
    ratio = args.ratio

    print(f"src_dir = {src_dir}")
    print(f"dst_dir = {dst_dir}")
    print(f"ratio   = {ratio}")

    split_dataset(src_dir, dst_dir, ratio)


def extract_args(args=None) -> object:
    """
    Argument parser

    PYTHONPATH="$PWD" python ./marie/coco_funsd_converter.py --mode test --step augment --strip_file_name_path true --dir ~/dataset/private/corr-indexer --config ~/dataset/private/corr-indexer/config.json --aug-count 2
    """
    parser = argparse.ArgumentParser(
        prog="coco_funsd_converter", description="COCO to FUNSD conversion utility"
    )

    subparsers = parser.add_subparsers(
        dest="command", help="Commands to run", required=True
    )

    convert_parser = subparsers.add_parser(
        "convert", help="Convert documents from COCO to FUNSD-Like intermediate format"
    )

    convert_parser.set_defaults(func=default_convert)

    convert_parser.add_argument(
        "--mode",
        required=True,
        type=str,
        default="train",
        help="Conversion mode : train/test/validate/etc",
    )

    convert_parser.add_argument(
        "--mode-suffix",
        required=False,
        type=str,
        default="-deck-raw",
        help="Suffix for the mode",
    )

    convert_parser.add_argument(
        "--strip_file_name_path",
        required=True,
        # type=bool,
        # action='store_true',
        type=lambda x: bool(distutils.util.strtobool(x)),
        default=False,
        help="Should full image paths be striped from annotations file",
    )

    convert_parser.add_argument(
        "--dir",
        required=True,
        type=str,
        default="~/dataset/ds-001/indexer",
        help="Base data directory",
    )

    convert_parser.add_argument(
        "--dir-converted",
        required=False,
        type=str,
        default="./converted",
        help="Converted data directory",
    )

    convert_parser.add_argument(
        "--config",
        required=True,
        type=str,
        default="./config.json",
        help="Configuration file used for conversion",
    )

    decorate_parser = subparsers.add_parser(
        "decorate", help="Decorate documents(Box detection, ICR)"
    )
    decorate_parser.set_defaults(func=default_decorate)

    decorate_parser.add_argument(
        "--mode",
        required=True,
        type=str,
        default="train",
        help="Conversion mode : train/test/validate/etc",
    )

    decorate_parser.add_argument(
        "--dir",
        required=True,
        type=str,
        help="Base dataset directory where the document for decorating resize",
    )

    augment_parser = subparsers.add_parser("augment", help="Augment documents")
    augment_parser.set_defaults(func=default_augment)

    augment_parser.add_argument(
        "--mode",
        required=True,
        type=str,
        help="Conversion mode : train/test/validate/etc",
    )

    augment_parser.add_argument(
        "--dir",
        required=True,
        type=str,
        help="Source directory",
    )

    augment_parser.add_argument(
        "--dir-output",
        default="./augmented",
        type=str,
        help="Destination directory",
    )

    augment_parser.add_argument(
        "--count",
        required=True,
        type=int,
        help="Number of augmentations per annotation",
    )

    rescale_parser = subparsers.add_parser(
        "rescale", help="Rescale/Normalize documents to be used by UNILM"
    )
    rescale_parser.set_defaults(func=default_rescale)

    rescale_parser.add_argument(
        "--mode",
        required=True,
        type=str,
        help="Conversion mode : train/test/validate/etc",
    )

    rescale_parser.add_argument(
        "--dir",
        required=True,
        type=str,
        help="Source directory",
    )

    rescale_parser.add_argument(
        "--dir-output",
        default="./rescaled",
        type=str,
        help="Destination directory",
    )

    rescale_parser.add_argument(
        "--suffix",
        default="-augmented",
        type=str,
        help="Suffix to append to the source directory",
    )

    visualize_parser = subparsers.add_parser("visualize", help="Visualize documents")
    visualize_parser.set_defaults(func=default_visualize)

    visualize_parser.add_argument(
        "--dir",
        required=True,
        type=str,
        help="Source directory",
    )

    visualize_parser.add_argument(
        "--dir-output",
        default="/tmp/visualize",
        type=str,
        help="Destination directory",
    )

    visualize_parser.add_argument(
        "--config",
        type=str,
        default="./visualize-config.json",
        help="Configuration file used for conversion",
    )

    split_parser = subparsers.add_parser(
        "split", help="Split COCO dataset into train/test"
    )
    split_parser.set_defaults(func=default_split)

    split_parser.add_argument(
        "--dir",
        required=True,
        type=str,
        help="Source directory",
    )

    split_parser.add_argument(
        "--dir-output",
        default="/tmp/split",
        type=str,
        help="Destination directory",
    )

    split_parser.add_argument(
        "--ratio",
        default=0.8,
        type=float,
        help="Destination directory",
    )

    convert_all_parser = subparsers.add_parser(
        "convert-all",
        help="Run all conversion phases[convert,decorate,augment,rescale] using most defaults.",
    )

    convert_all_parser.set_defaults(func=default_all_steps)

    convert_all_parser.add_argument(
        "--mode",
        required=True,
        type=str,
        default="train",
        help="Conversion mode : train/test/validate/etc",
    )

    convert_all_parser.add_argument(
        "--mode-suffix",
        required=False,
        type=str,
        default="-deck-raw",
        help="Suffix for the mode",
    )

    convert_all_parser.add_argument(
        "--strip_file_name_path",
        required=True,
        # type=bool,
        # action='store_true',
        type=lambda x: bool(distutils.util.strtobool(x)),
        default=False,
        help="Should full image paths be striped from annotations file",
    )

    convert_all_parser.add_argument(
        "--aug-count",
        required=True,
        type=int,
        help="Number of augmentations per annotation",
    )

    convert_all_parser.add_argument(
        "--dir",
        required=True,
        type=str,
        default="~/dataset/ds-001/indexer",
        help="Base data directory",
    )

    convert_all_parser.add_argument(
        "--config",
        required=True,
        type=str,
        default="./config.json",
        help="Configuration file used for conversion",
    )

    try:
        return parser.parse_args(args) if args else parser.parse_args()
    except:
        parser.print_help()
        sys.exit(0)


if __name__ == "__main__":
    args = extract_args()
    print("-" * 120)
    print(args)
    print("-" * 120)

    args.func(args)
