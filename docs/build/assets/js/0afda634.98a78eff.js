"use strict";(self.webpackChunkmare_ai=self.webpackChunkmare_ai||[]).push([[6752],{28453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>o});var i=t(96540);const a={},r=i.createContext(a);function s(e){const n=i.useContext(r);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),i.createElement(r.Provider,{value:n},e.children)}},39755:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>o,default:()=>m,frontMatter:()=>s,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"models/named-entity-recognition","title":"Named entity recognition","description":"Extracting Named Entity Recognition / Key Value pair extraction","source":"@site/docs/models/named-entity-recognition.md","sourceDirName":"models","slug":"/models/named-entity-recognition","permalink":"/docs/models/named-entity-recognition","draft":false,"unlisted":false,"editUrl":"https://github.com/marieai/marie-ai/tree/main/docs/docs/models/named-entity-recognition.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Model Zoo","permalink":"/docs/models/model-zoo"},"next":{"title":"Registry","permalink":"/docs/models/model-registry"}}');var a=t(74848),r=t(28453);const s={sidebar_position:1},o="Named entity recognition",d={},l=[{value:"Configuration",id:"configuration",level:2},{value:"Examples",id:"examples",level:2},{value:"Executor setup",id:"executor-setup",level:3},{value:"Complete NER example",id:"complete-ner-example",level:3},{value:"Fine-Tuning LayoutLM v3",id:"fine-tuning-layoutlm-v3",level:2},{value:"Setup Development Environment",id:"setup-development-environment",level:3},{value:"Load and prepare dataset",id:"load-and-prepare-dataset",level:3},{value:"Directory structure",id:"directory-structure",level:4},{value:"Merge/Split multiple datasets (unilm/dit/tools)",id:"mergesplit-multiple-datasets-unilmdittools",level:2},{value:"Utility usage",id:"utility-usage",level:4},{value:"command : convert",id:"command--convert",level:4},{value:"command : decorate",id:"command--decorate",level:4},{value:"command : augment",id:"command--augment",level:4},{value:"command : rescale",id:"command--rescale",level:4},{value:"command : convert-all",id:"command--convert-all",level:4},{value:"command : visualize",id:"command--visualize",level:4},{value:"command : split",id:"command--split",level:4},{value:"Configuration",id:"configuration-1",level:4},{value:"Duplicate Mapping",id:"duplicate-mapping",level:4},{value:"Key-Value validation",id:"key-value-validation",level:4},{value:"Linking Fields",id:"linking-fields",level:4},{value:"Training",id:"training",level:3},{value:"DIT Bounding Boxes",id:"dit-bounding-boxes",level:2},{value:"Reference",id:"reference",level:2}];function c(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"named-entity-recognition",children:"Named entity recognition"})}),"\n",(0,a.jsx)(n.p,{children:"Extracting Named Entity Recognition / Key Value pair extraction"}),"\n",(0,a.jsx)(n.h2,{id:"configuration",children:"Configuration"}),"\n",(0,a.jsx)(n.h2,{id:"examples",children:"Examples"}),"\n",(0,a.jsx)(n.h3,{id:"executor-setup",children:"Executor setup"}),"\n",(0,a.jsx)(n.p,{children:"Basic executor setup and inference."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from marie.executor import NerExtractionExecutor\nfrom marie.utils.image_utils import hash_file\n\n# setup executor\nmodels_dir = "/mnt/data/models/"\nexecutor = NerExtractionExecutor(models_dir)\n\nimg_path = "/tmp/sample.png"\nchecksum = hash_file(img_path)\n\n# invoke executor\ndocs = None\nkwa = {"checksum": checksum, "img_path": img_path}\nresults = executor.extract(docs, **kwa)\n\nprint(results)\n'})}),"\n",(0,a.jsx)(n.h3,{id:"complete-ner-example",children:"Complete NER example"}),"\n",(0,a.jsxs)(n.p,{children:["Setup Named Entity Recognition executor ",(0,a.jsx)(n.code,{children:"NerExtractionExecutor"})," and storage backend ",(0,a.jsx)(n.code,{children:"PostgreSQLStorage"})]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import glob\nimport os\nfrom typing import Dict\n\nimport transformers\n\nfrom marie.conf.helper import storage_provider_config, load_yaml\nfrom marie.executor.ner import NerExtractionExecutor\nfrom marie.executor.storage.PostgreSQLStorage import PostgreSQLStorage\nfrom marie.logging_core.profile import TimeContext\nfrom marie.registry.model_registry import ModelRegistry\nfrom marie.utils.image_utils import hash_file, hash_bytes\nfrom marie.utils.json import store_json_object\nfrom marie.constants import __model_path__, __config_dir__\nfrom marie import (\n    Document,\n    DocumentArray,\n)\n\n\ndef process_file(\n    executor: NerExtractionExecutor,\n    img_path: str,\n    storage_enabled: bool,\n    storage_conf: Dict[str, str],\n):\n    with TimeContext(f"### extraction info"):\n        filename = img_path.split("/")[-1].replace(".png", "")\n        checksum = hash_file(img_path)\n        docs = None\n        kwa = {"checksum": checksum, "img_path": img_path}\n        payload = executor.extract(docs, **kwa)\n        print(payload)\n        store_json_object(payload, f"/tmp/tensors/json/{filename}.json")\n\n        if storage_enabled:\n            storage = PostgreSQLStorage(\n                hostname=storage_conf["hostname"],\n                port=int(storage_conf["port"]),\n                username=storage_conf["username"],\n                password=storage_conf["password"],\n                database=storage_conf["database"],\n                table="check_ner_executor",\n            )\n\n            dd2 = DocumentArray([Document(content=payload)])\n            storage.add(dd2, {"ref_id": filename, "ref_type": "filename"})\n\n        return payload\n\n\ndef process_dir(\n    executor: NerExtractionExecutor,\n    image_dir: str,\n    storage_enabled: bool,\n    conf: Dict[str, str],\n):\n    for idx, img_path in enumerate(glob.glob(os.path.join(image_dir, "*.*"))):\n        try:\n            process_file(executor, img_path, storage_enabled, conf)\n        except Exception as e:\n            print(e)\n            # raise e\n\n\nif __name__ == "__main__":\n    # pip install git+https://github.com/huggingface/transformers\n    # 4.18.0  -> 4.21.0.dev0 : We should pin it to this version\n    print(transformers.__version__)\n    _name_or_path = "rms/layoutlmv3-large-corr-ner"\n    kwargs = {"__model_path__": __model_path__}\n    _name_or_path = ModelRegistry.get(_name_or_path, **kwargs)\n\n    print(__config_dir__)\n    # Load config\n    config_data = load_yaml(os.path.join(__config_dir__, "marie-debug.yml"))\n    storage_conf = storage_provider_config("postgresql", config_data)\n    executor = NerExtractionExecutor(_name_or_path)\n\n    single_file = True\n    img_path = f"/home/greg/tmp/image5839050414130576656-0.tif"\n\n    if single_file:\n        process_file(executor, img_path, True, storage_conf)\n    else:\n        process_dir(executor, img_path, True, storage_conf)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"fine-tuning-layoutlm-v3",children:"Fine-Tuning LayoutLM v3"}),"\n",(0,a.jsx)(n.p,{children:"From annotation to training and inference"}),"\n",(0,a.jsx)(n.h3,{id:"setup-development-environment",children:"Setup Development Environment"}),"\n",(0,a.jsxs)(n.p,{children:["There are two separate environments one using ",(0,a.jsx)(n.code,{children:"pip"})," for Marie-AI and other using ",(0,a.jsx)(n.code,{children:"conda"})," for UniLM. We could mix them\nhowever there are different dependencies needed for UniLM and Marie so it is safer to keep them segregated. Additionally,\nthere is no need to use ",(0,a.jsx)(n.code,{children:"conda"})," as this could have been setup with ",(0,a.jsx)(n.code,{children:"pip"})," as well."]}),"\n",(0,a.jsx)(n.h3,{id:"load-and-prepare-dataset",children:"Load and prepare dataset"}),"\n",(0,a.jsxs)(n.p,{children:["Data prep is done from tools from ",(0,a.jsx)(n.code,{children:"marie-ai"}),", to setup development environment follow ",(0,a.jsx)(n.a,{href:"/docs/getting-started/installation",children:"getting started guide"}),".\nData is labeled using ",(0,a.jsx)(n.a,{href:"https://github.com/opencv/cvat",children:"Computer Vision Annotation Tool (CVAT)"})," in ",(0,a.jsx)(n.a,{href:"https://cocodataset.org/#format-data",children:"COCO Dataset format"}),"."]}),"\n",(0,a.jsxs)(n.p,{children:["Convert CVAT annotated COCO dataset into ",(0,a.jsx)(n.a,{href:"https://guillaumejaume.github.io/FUNSD/",children:"FUNSD"})," compatible format for finetuning models."]}),"\n",(0,a.jsx)(n.h4,{id:"directory-structure",children:"Directory structure"}),"\n",(0,a.jsx)(n.p,{children:"Each directory should be in COCO 1.0 format when exporting from CVAT."}),"\n",(0,a.jsxs)(n.p,{children:["Example structure for ",(0,a.jsx)(n.code,{children:"test"})," and ",(0,a.jsx)(n.code,{children:"train"})," modes, by default the data suffix of ",(0,a.jsx)(n.code,{children:"-deck-raw"})," will be added to the mode to\ncreate a directory name."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:"~/dataset/indexer\n\u251c\u2500\u2500 test-deck-raw\n\u2502   \u251c\u2500\u2500 annotations\n\u2502   \u2502 \u2514\u2500\u2500 instances_default.json\n\u2502   \u2514\u2500\u2500 images\n\u2502       \u251c\u2500\u2500 157303757_0.png\n\u2502       \u251c\u2500\u2500 157303757_2.png\n\u2502       \u2514\u2500\u2500 157303757_4.png\n\u251c\u2500\u2500 train-deck-raw\n\u2502   \u251c\u2500\u2500 annotations\n\u2502   \u2502   \u2514\u2500\u2500 instances_default.json\n\u2502   \u2514\u2500\u2500 images\n\u2502       \u251c\u2500\u2500 157303758_0.png\n\u2502       \u251c\u2500\u2500 157303758_2.png\n\u2502       \u2514\u2500\u2500 157303758_4.png\n\u2514\u2500\u2500 validation\n"})}),"\n",(0,a.jsx)(n.h2,{id:"mergesplit-multiple-datasets-unilmdittools",children:"Merge/Split multiple datasets (unilm/dit/tools)"}),"\n",(0,a.jsxs)(n.p,{children:["When we have multiple datasets that we want to merge into one dataset we can use ",(0,a.jsx)(n.code,{children:"unilm/dit/tools"})," script.\nThis step is not needed if we are working with a single dataset that we exported from CVAT directly.\nHowevere if we have multiple datasets that we want to merge into one dataset we can use ",(0,a.jsx)(n.code,{children:"unilm/dit/tools"})," script to have valid Train/Test split."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:"~/unilm/dit/tools\n\u251c\u2500\u2500 cocosplit.py\n\u2514\u2500\u2500 merge_dir.py\n"})}),"\n",(0,a.jsx)(n.p,{children:"Merge multiple datasets into one."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:"python ./merge_dir.py --src_dir ~/datasets/private/corr-indexer/coco_annotations --output_file ~/datasets/private/corr-indexer/converted/instances_default_merged.json\n"})}),"\n",(0,a.jsx)(n.p,{children:"Split into Train/Test datasets"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:"python ./cocosplit.py  ~/datasets/private/corr-indexer/converted/instances_default_merged.json ~/datasets/private/corr-indexer/converted/instances_training.json ~/datasets/private/corr-indexer/converted/instances_test.json -s .8\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Expected output will be two files ",(0,a.jsx)(n.code,{children:"instances_training.json"})," and ",(0,a.jsx)(n.code,{children:"instances_test.json"})," that will be used for training and testing.\nYou should see output similar to this:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:"\n\u250c\u2500\u2500 unilm/dit/tools  \ue606 v3.10.12(marie) 5 months ago took 1s   \n\u2514\u2500\u03bb python ./merge_dir.py --src_dir ~/datasets/private/corr-indexer/coco_annotations --output_file ~/datasets/private/corr-indexer/converted/merged.json\nNamespace(src_dir='/home/greg/datasets/private/corr-indexer/coco_annotations', output_file='/home/greg/datasets/private/corr-indexer/converted/merged.json')\nFound 6 files in /home/greg/datasets/private/corr-indexer/coco_annotations\nOutput file: /home/greg/datasets/private/corr-indexer/converted/merged.json\nMerging 6 files\nMerging : 1\n 47%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                                                                        | 1279/2726 [00:01<00:01, 805.32it/s]\n 47%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                                                                        | 1279/2726 [00:03<00:04, 337.89it/s]\nMerging : 2\n 49%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                    | 1392/2840 [00:03<00:03, 367.85it/s]\n 49%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                    | 1392/2840 [00:04<00:05, 278.87it/s]\nMerging : 3\n 51%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                                                               | 1523/2977 [00:05<00:05, 280.48it/s]\n 51%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                                                               | 1523/2977 [00:06<00:06, 218.39it/s]\nMerging : 4\n 48%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                                                                                      | 1668/3497 [00:07<00:08, 225.62it/s]\n 48%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                                                                                      | 1668/3497 [00:08<00:09, 189.33it/s]\nMerging : 5\n 48%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                                                                     | 1693/3522 [00:09<00:09, 186.61it/s]\n 48%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                                                                     | 1693/3522 [00:09<00:10, 180.44it/s]\n\n\u250c\u2500\u2500 unilm/dit/tools  \ue606 v3.10.12(marie) 5 months ago took 1m41s   \n\u2514\u2500\u03bb code .\n\n\u250c\u2500\u2500 unilm/dit/tools  \ue606 v3.10.12(marie) 5 months ago    \n\u2514\u2500\u03bb python ./cocosplit.py  ~/datasets/private/corr-indexer/converted/instances_default_merged.json ~/datasets/private/corr-indexer/converted/instances_training.json ~/datasets/private/corr-indexer/converted/instances_test.json -s .8\nSaved 35930 entries in /home/greg/datasets/private/corr-indexer/converted/instances_training.json and 8894 in /home/greg/datasets/private/corr-indexer/converted/instances_test.json\n"})}),"\n",(0,a.jsx)(n.p,{children:"After split the JSON files should be moved to following directories."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:"~/dataset/indexer\n\u251c\u2500\u2500 train-deck-raw\n\u251c\u2500\u2500 test-deck-raw\n"})}),"\n",(0,a.jsx)(n.h1,{id:"visualize-the-dataset-using-detectron2-this-needs-to-be-updated",children:"visualize the dataset using DETECTRON2 (THIS NEEDS TO BE UPDATED)"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:"python -m detectron2.data.datasets.coco ~/datasets/private/corr-indexer/test-deck-raw/annotations/instances_default.json ~/datasets/private/corr-indexer/test-deck-raw/images  corr_indexer_dataset_test\n"})}),"\n",(0,a.jsx)(n.p,{children:"Activate our marie-ai environment."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:"cd ~/dev/marie-ai\nsource ./venv/bin/activate\n"})}),"\n",(0,a.jsx)(n.p,{children:"The script performs few basic steps."}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"convert   : Convert COCO to FUNSD like format"}),"\n",(0,a.jsx)(n.li,{children:"decorate  : Text Box detection, ICR/OCR"}),"\n",(0,a.jsx)(n.li,{children:"augment   : Data augmentation"}),"\n",(0,a.jsx)(n.li,{children:"rescale   : Rescale/Normalize documents to be used by UNILM"}),"\n",(0,a.jsx)(n.li,{children:"visualize : Visualize documents"}),"\n",(0,a.jsx)(n.li,{children:"split     : Split COCO dataset into train/test"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Each command can be invoked separately, but initially they need to be invoked in following order if you don't have already\ngenerated intermediate assets."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:"convert -> decorate -> augment -> rescale\n"})}),"\n",(0,a.jsx)(n.h4,{id:"utility-usage",children:"Utility usage"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:"usage: coco_funsd_converter [-h] {convert,decorate,augment,rescale,visualize,split} ...\n\nCOCO to FUNSD conversion utility\n\npositional arguments:\n  {convert,decorate,augment,rescale,visualize,split,convert-all}\n                        Commands to run\n    convert             Convert documents from COCO to FUNSD-Like intermediate format\n    decorate            Decorate documents(Box detection, ICR)\n    augment             Augment documents\n    rescale             Rescale/Normalize documents to be used by UNILM\n    visualize           Visualize documents\n    split               Split COCO dataset into train/test\n    convert-all         Run all conversion phases[convert,decorate,augment,rescale] using most defaults.\n\noptional arguments:\n  -h, --help            show this help message and exit\n\n"})}),"\n",(0,a.jsx)(n.h4,{id:"command--convert",children:"command : convert"}),"\n",(0,a.jsx)(n.p,{children:"Convert documents from COCO to FUNSD-Like intermediate format"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:"usage: coco_funsd_converter convert [-h] --mode MODE [--mode-suffix MODE_SUFFIX] --strip_file_name_path STRIP_FILE_NAME_PATH --dir DIR [--dir_converted DIR_CONVERTED] [--dir_augmented DIR_AUGMENTED]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --mode MODE           Conversion mode : train/test/validate/etc\n  --mode-suffix MODE_SUFFIX\n                        Suffix for the mode\n  --strip_file_name_path STRIP_FILE_NAME_PATH\n                        Should full image paths be striped from annotations file\n  --dir DIR             Base data directory\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"usage"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:' PYTHONPATH="$PWD" python ./marie/coco_funsd_converter.py convert --mode test \\\n --strip_file_name_path true --dir ~/dataset/private/corr-indexer \\\n --config ~/dataset/private/corr-indexer/config.json\n'})}),"\n",(0,a.jsxs)(n.p,{children:["When we have datasets that don't line up with our annotations ",(0,a.jsx)(n.code,{children:"file_image"})," we can use ",(0,a.jsx)(n.code,{children:"strip_file_name_path"})," argument\nto strip the image path and use the image file name only."]}),"\n",(0,a.jsx)(n.p,{children:"Default generated folder structure will look as follows (using defaults):"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:"/indexer/output\n\u2514\u2500\u2500 dataset\n    \u2514\u2500\u2500 test\n        \u251c\u2500\u2500 annotations_tmp    \n        \u2514\u2500\u2500 images\n"})}),"\n",(0,a.jsx)(n.h4,{id:"command--decorate",children:"command : decorate"}),"\n",(0,a.jsxs)(n.p,{children:["This step post-processed the data that have been generated via ",(0,a.jsx)(n.code,{children:"convert"})," command and will be created in ",(0,a.jsx)(n.code,{children:"/indexer/output/dataset"})]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"usage"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:'PYTHONPATH="$PWD" python ./marie/coco_funsd_converter.py decorate --mode test --dir ~/dataset/private/corr-indexer/output/dataset\n'})}),"\n",(0,a.jsxs)(n.p,{children:["After the command finished output directory will have a new folder add called ",(0,a.jsx)(n.code,{children:"annotations"})," which contain boxes and ICR."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:"/indexer/output\n\u2514\u2500\u2500 dataset\n    \u2514\u2500\u2500 test\n        \u251c\u2500\u2500 annotations        <------------ \n        \u251c\u2500\u2500 annotations_tmp\n        \u2514\u2500\u2500 images\n"})}),"\n",(0,a.jsx)(n.h4,{id:"command--augment",children:"command : augment"}),"\n",(0,a.jsx)(n.p,{children:"Augmentation is not necessary however it provides additional way to introduce variability into datasets that are small."}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"usage"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:'PYTHONPATH="$PWD" python ./marie/coco_funsd_converter.py augment --mode test \\\n--dir ~/dataset/private/corr-indexer/output/dataset --count 1\n'})}),"\n",(0,a.jsx)(n.p,{children:"After the script is run our directory structure will look as follows:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:"/indexer/output\n\u2514\u2500\u2500 dataset\n    \u251c\u2500\u2500 test\n    \u2502\xa0\xa0 \u251c\u2500\u2500 annotations\n    \u2502\xa0\xa0 \u251c\u2500\u2500 annotations_tmp\n    \u2502\xa0\xa0 \u2514\u2500\u2500 images\n    \u2514\u2500\u2500 test-augmented              <------------ \n        \u251c\u2500\u2500 annotations\n        \u2514\u2500\u2500 images\n"})}),"\n",(0,a.jsx)(n.h4,{id:"command--rescale",children:"command : rescale"}),"\n",(0,a.jsx)(n.p,{children:"Augmentation is not necessary however it provides additional way to introduce variability into datasets that are small."}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"usage"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:'PYTHONPATH="$PWD" python ./marie/coco_funsd_converter.py rescale --mode test \\\n--dir ~/dataset/private/corr-indexer/output/dataset\n'})}),"\n",(0,a.jsx)(n.p,{children:"After the script is run our directory structure will look as follows:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:"/indexer/output\n\u2514\u2500\u2500 dataset\n    \u251c\u2500\u2500 test\n    \u2502\xa0\xa0 \u251c\u2500\u2500 annotations\n    \u2502\xa0\xa0 \u251c\u2500\u2500 annotations_tmp\n    \u2502\xa0\xa0 \u2514\u2500\u2500 images\n    \u251c\u2500\u2500 test-augmented\n    \u2502\xa0\xa0 \u251c\u2500\u2500 annotations\n    \u2502\xa0\xa0 \u2514\u2500\u2500 images\n    \u2514\u2500\u2500 test-rescaled             <------------ \n        \u251c\u2500\u2500 annotations\n        \u2514\u2500\u2500 images\n"})}),"\n",(0,a.jsx)(n.h4,{id:"command--convert-all",children:"command : convert-all"}),"\n",(0,a.jsx)(n.p,{children:"Run all conversion phases[convert,decorate,augment,rescale] using most defaults. This is the fastest way to test your\nmodel and make sure that everything is configured correctly."}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"usage"}),"\n",(0,a.jsx)(n.a,{target:"_blank","data-noBrokenLinkCheck":!0,href:t(90328).A+"",children:"181312401_2.json"})]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:'PYTHONPATH="$PWD" python ./tools/coco_funsd_augmenter.py convert-all --mode test  --strip_file_name_path true --aug-count 2 --dir ~/datasets/private/corr-indexer  --config ~/datasets/private/corr-indexer/config.json\n'})}),"\n",(0,a.jsx)(n.h4,{id:"command--visualize",children:"command : visualize"}),"\n",(0,a.jsx)(n.p,{children:"Command for visualizing FUNDS like datasets."}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"usage"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:' PYTHONPATH="$PWD" python ./tools/coco_funsd_augmenter.py visualize --dir ~/datasets/private/corr-indexer/output/dataset/test-augmented  --config ~/datasets/private/corr-indexer/visualize-config.json\n'})}),"\n",(0,a.jsx)(n.p,{children:"Configuration is optional but if provided we will have constant label colors across images."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-json",children:'{\n "label2color": {\n  "pan": "blue",\n  "pan_answer": "green",\n  "dos": "orange"\n }\n}\n'})}),"\n",(0,a.jsx)(n.h4,{id:"command--split",children:"command : split"}),"\n",(0,a.jsx)(n.p,{children:"Split COCO dataset for training and test."}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"usage"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:'PYTHONPATH="$PWD" python ./marie/coco_funsd_augmenter.py split --dir ~/datasets/private/corr-indexer/output/dataset/test-rescaled --ratio .8\n'})}),"\n",(0,a.jsx)(n.h4,{id:"configuration-1",children:"Configuration"}),"\n",(0,a.jsxs)(n.p,{children:["Configuration for the tool is defined via ",(0,a.jsx)(n.code,{children:"--config"})," attribute and file is in JSON format."]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Validation"})}),"\n",(0,a.jsx)(n.p,{children:"There are couple different validations that will be performed."}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Duplicate Mapping (Validation is enabled by default)"}),"\n",(0,a.jsx)(n.li,{children:"Key / Value aka Question/Answer in FUNSD dataset"}),"\n"]}),"\n",(0,a.jsx)(n.h4,{id:"duplicate-mapping",children:"Duplicate Mapping"}),"\n",(0,a.jsx)(n.p,{children:"Base validation to ensure that we don't have duplicate fields mappings."}),"\n",(0,a.jsx)(n.admonition,{title:"Duplicate field check",type:"warning",children:(0,a.jsx)(n.p,{children:"Duplicate pair found for image_id[25] : member_name, 4, sample.png"})}),"\n",(0,a.jsxs)(n.p,{children:["This validation message tells us that CVAT image_id 25 (zero based) and field ",(0,a.jsx)(n.code,{children:"member_name"})," had 4 duplicate\nvalues present on given image."]}),"\n",(0,a.jsx)(n.h4,{id:"key-value-validation",children:"Key-Value validation"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.code,{children:"question_answer_map"})," this fields maps KEY => Value, and it is one-to-one mapping, this mapping can be any field that\nwas defined in CVAT."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-json",metastring:'title="Configuration fragment"',children:'{\n    "question_answer_map" : {\n        "member_name": "member_name_answer",\n        "member_number": "member_number_answer",\n        "pan": "pan_answer",\n        "dos": "dos_answer",\n        "patient_name": "patient_name_answer"\n    }\n}\n'})}),"\n",(0,a.jsx)(n.p,{children:"When validation fails for above definition we will receive message:"}),"\n",(0,a.jsx)(n.admonition,{title:"Missing mapping",type:"warning",children:(0,a.jsx)(n.p,{children:"Missing mapping\nPair not found for image_id[25] : member_name [1] MISSING -> member_name_answer [8]"})}),"\n",(0,a.jsxs)(n.p,{children:["This validation message tells us that CVAT image_id 25 (zero based) and field ",(0,a.jsx)(n.code,{children:"member_name"})," is missing corresponding\n",(0,a.jsx)(n.code,{children:"member_name_answer"})," field."]}),"\n",(0,a.jsx)(n.h4,{id:"linking-fields",children:"Linking Fields"}),"\n",(0,a.jsx)(n.p,{children:"As we are following FUNSD dataset format we have a definition for linking fields from our COCO dataset."}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.code,{children:"id_map"})," config key maps arbitrary ",(0,a.jsx)(n.code,{children:"key"})," to and ",(0,a.jsx)(n.code,{children:"id"}),".  The ",(0,a.jsx)(n.code,{children:"id"})," could be the same ",(0,a.jsx)(n.code,{children:"id"})," as used in CVAT."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-json",metastring:'title="Configuration fragment"',children:'  {\n   "id_map" : {\n        "member_name": 0,\n        "member_name_answer": 1,\n        "member_number": 2,\n        "member_number_answer": 3,\n        "pan": 4,\n        "pan_answer": 5,\n        "dos": 6,\n        "dos_answer": 7,\n        "patient_name": 8,\n        "patient_name_answer": 9,\n    }\n}\n'})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.code,{children:"link_map"})," config key maps arbitrary ",(0,a.jsx)(n.code,{children:"key"})," to ",(0,a.jsx)(n.code,{children:"id"})," and links the two fields together."]}),"\n",(0,a.jsxs)(n.p,{children:["This tells us that ",(0,a.jsx)(n.code,{children:"member_name"})," is linked to ",(0,a.jsx)(n.code,{children:"[member_name, member_name_answer]"})," and vice versa as the mapping is\nbidirectional."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-json",children:'  {\n    "member_name": [0,1],\n    "member_name_answer": [0,1]\n  }\n'})}),"\n",(0,a.jsxs)(n.p,{children:["To declare a field that is not linked to anything we give it a value of ",(0,a.jsx)(n.code,{children:"-1"})]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-json",children:'  {\n    "paragraph": [-1]\n  }\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-json",metastring:'title="Configuration fragment"',children:'"link_map" : {\n  "member_name": [\n    0,\n    1\n  ],\n  "member_name_answer": [\n    0,\n    1\n  ],\n  "member_number": [\n    2,\n    3\n  ],\n  "member_number_answer": [\n    2,\n    3\n  ],\n  "paragraph": [\n    -1\n  ],\n  "greeting": [\n    -1\n  ]\n}\n\n'})}),"\n",(0,a.jsx)(n.h3,{id:"training",children:"Training"}),"\n",(0,a.jsxs)(n.p,{children:["Clone UniLM: Unified pre-training for language understanding (NLU) and generation (NLG) project from following repo ",(0,a.jsx)(n.a,{href:"https://github.com/gregbugaj/unilm.git",children:"https://github.com/gregbugaj/unilm.git"})," which is a fork of ",(0,a.jsx)(n.a,{href:"https://github.com/microsoft/unilm.git",children:"https://github.com/microsoft/unilm.git"}),"\nFork is kept in sync, but it does contain additional changes."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:"cd ~/dev\ngit clone https://github.com/gregbugaj/unilm.git\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Activate your PyTorch environment that will be used to train ",(0,a.jsx)(n.code,{children:"layoutlmv3"})]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:"conda env list\nconda activate layoutlmv3\ncd ~/dev/unilm/\n"})}),"\n",(0,a.jsx)(n.p,{children:"To start trainin we can use the following script"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:"screen\nCUDA_LAUNCH_BLOCKING=1 CUDA_VISIBLE_DEVICES=1 python ./train.py\n"})}),"\n",(0,a.jsx)(n.h2,{id:"dit-bounding-boxes",children:"DIT Bounding Boxes"}),"\n",(0,a.jsx)(n.p,{children:"Convert"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:' PYTHONPATH="$PWD" python ./tools/coco_funsd_converter.py convert --mode train  --strip_file_name_path true --dir ~/datasets/funsd_bboxes  --config ~/datasets/funsd_bboxes/config.json\n'})}),"\n",(0,a.jsx)(n.p,{children:"Rescale"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:"PYTHONPATH=\"$PWD\" python ./tools/coco_funsd_converter.py rescale --mode train --dir ~/datasets/funsd_bboxes/output/dataset --suffix ''\n"})}),"\n",(0,a.jsx)(n.h2,{id:"reference",children:"Reference"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://arxiv.org/abs/2204.08387",children:"LayoutLMv3: Multi-modal Pre-training for Visually-Rich Document Understanding"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://guillaumejaume.github.io/FUNSD/",children:"FUNSD: Form Understanding in Noisy Scanned Documents"})}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},90328:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/files/181312401_2-11ca565287c8ee157d36fc3d2b414769.json"}}]);