# ðŸ“Š Benchmarking sglang with `wrk`

This guide shows how to benchmark your **Qwen/QwQ-32B deployment** using [`wrk`](https://github.com/wg/wrk), a modern
HTTP benchmarking tool.
Weâ€™ll simulate **multiâ€‘user workloads** with both short and long prompts.

---

## ðŸ”¹ 1. Install `wrk`

### Ubuntu/Debian

```bash
sudo apt-get update
sudo apt-get install wrk -y
```

### Build from source (latest version)

```bash
sudo apt-get install build-essential libssl-dev git -y
git clone https://github.com/wg/wrk.git
cd wrk
make
sudo cp wrk /usr/local/bin
wrk --version
```

---

## ðŸ”¹ 2. Lua Payload Scripts

Two Lua scripts are provided:

### `sglang_post.lua` (short prompt)

### `sglang_long_post.lua` (long prompt: \~2048 in / 1024 out)

---

## ðŸ”¹ 3. Running Benchmarks

### Against single container (port `8000`)

```bash
# Short prompt, light load
wrk -t8 -c50 -d30s -s sglang_post.lua http://localhost:8000/v1/chat/completions

# Long prompt, heavier load
wrk -t12 -c100 -d60s -s sglang_long_post.lua http://localhost:8000/v1/chat/completions
```

### Against loadâ€‘balanced multiâ€‘container (port `8080`)

```bash
wrk -t12 -c300 -d60s -s sglang_post.lua http://localhost:8080/v1/chat/completions
```

---

## ðŸ”¹ 4. Test Profiles

We recommend three levels of testing:

* **Light load** â†’ `-c50 -d30s` (simulate \~50 users)
* **Medium load** â†’ `-c300 -d60s` (simulate \~300 users)
* **Heavy stress** â†’ `-c1000 -d90s` (simulate \~1000 users)

Example (medium load, long prompts):

```bash
wrk -t16 -c300 -d60s -s sglang_long_post.lua http://localhost:8000/v1/chat/completions
```

---

## ðŸ”¹ 5. What to Look At

`wrk` reports:

* **Requests/sec** â†’ overall throughput
* **Latency** (avg, stdev, 95th/99th percentile) â†’ multiâ€‘user response times
* **Transfer/sec** â†’ approximate bandwidth usage

Compare across:

* **Single 8â€‘GPU container (TP=8)** â†’ lowest latency per request
* **Two 4â€‘GPU containers (TP=4 each, behind LB)** â†’ higher sustained throughput with many users

---

## ðŸ”¹ 6. Interpreting Results

* If **`wrk` req/sec << expected tokens/sec** from `sglang.bench_serving` â†’ bottleneck is in API batching/scheduling,
  not the GPUs.
* Use `sglang_post.lua` for **shortâ€‘chat workloads** (low latency).
* Use `sglang_long_post.lua` for **stress testing longâ€‘context generation**.

