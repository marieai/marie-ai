!MarieGateway
uses: MarieGateway
py_modules:
  - marie.serve.runtimes.gateway.marie

# Currently the parameters have to be passed in as runtime arguments from command line
# This will be changed in the future
# marie gateway --uses /mnt/data/marie-ai/config/service/gateway.yml --protocols HTTP GRPC --ports 51000 52000

#protocols:
#  - "GRPC"
#  - "HTTP"
#
#ports:
#  - 54000
#  - 55000

port_monitoring:
  - 57706

# Shared configuration
shared_config:
  storage: &storage
    psql: &psql_conf_shared
      provider: postgresql
      hostname: 127.0.0.1
      port: 5432
      username: postgres
      password: 123456
      database: postgres
      default_table: shared_docs

  message: &message
    amazon_mq :  &amazon_mq_conf_shared
      provider: amazon-rabbitmq
      hostname: ${{ ENV.AWS_MQ_HOSTNAME }}
      port: 15672
      username: ${{ ENV.AWS_MQ_USERNAME }}
      password: ${{ ENV.AWS_MQ_PASSWORD }}
      tls: True
      virtualhost: /

    rabbitmq :  &rabbitmq_conf_shared
      provider: rabbitmq
      hostname: ${{ ENV.RABBIT_MQ_HOSTNAME }}
      port: ${{ ENV.RABBIT_MQ_PORT }}
      username: ${{ ENV.RABBIT_MQ_USERNAME }}
      password: ${{ ENV.RABBIT_MQ_PASSWORD }}
      tls: False
      virtualhost: /

# this will be passed in as runtime arguments in kwargs
with:

  discovery: true
  discovery_host: 0.0.0.0
  discovery_port: 2379
  discovery_watchdog_interval: 2
  discovery_service_name: gateway/marie

  # Key Value Store
  kv_store_kwargs:
    <<: *psql_conf_shared
    default_table: kv_store_worker
    max_pool_size: 5
    max_connections: 5

    # Job Scheduler
  job_scheduler_kwargs:
    <<: *psql_conf_shared
    default_table: job_scheduler
    max_pool_size: 5
    max_connections: 5
    queue_names: [extract,classify,transform,load] # Queue names for the job scheduler to monitor

  # Toast event tracking system
  # It can be backed by Message Queue or Database backed
  toast:
    native:
      enabled: True
      path: /tmp/marie/events.json
    rabbitmq:
      <<: *rabbitmq_conf_shared
      enabled : True
    psql:
      <<: *psql_conf_shared
      default_table: event_tracking
      enabled : True

  # Document Storage
  # The storage service is used to store the data that is being processed
  # Storage can be backed by S3 compatible

  storage:
    # S3 configuration. Will be used only if value of backend is "s3"
    s3:
      enabled: True
      metadata_only: False # If True, only metadata will be stored in the storage backend
      # api endpoint to connect to. use AWS S3 or any S3 compatible object storage endpoint.
      endpoint_url: ${{ ENV.S3_ENDPOINT_URL }}
      # optional.
      # access key id when using static credentials.
      access_key_id: ${{ ENV.S3_ACCESS_KEY_ID }}
      # optional.
      # secret key when using static credentials.
      secret_access_key: ${{ ENV.S3_SECRET_ACCESS_KEY }}
      # Bucket name in s3
      bucket_name: ${{ ENV.S3_BUCKET_NAME }}
      # optional.
      # Example: "region: us-east-2"
      region: ${{ ENV.S3_REGION }}
      # optional.
      # enable if endpoint is http
      insecure: True
      # optional.
      # enable if you want to use path style requests
      addressing_style: path

    # postgresql configuration. Will be used only if value of backend is "psql"
    psql:
      <<: *psql_conf_shared
      default_table: store_metadata
      enabled : False

  # Authentication and Authorization configuration
  auth:
    keys:
      - name : service-A
        api_key : mas_0aPJ9Q9nUO1Ac1vJTfffXEXs9FyGLf9BzfYgZ_RaHm707wmbfHJNPQ
        enabled : True
        roles : [admin, user]

      - name : service-B
        api_key : mau_t6qDi1BcL1NkLI8I6iM8z1va0nZP01UQ6LWecpbDz6mbxWgIIIZPfQ
        enabled : True
        roles : [admin, user]
