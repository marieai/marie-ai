jtype: Flow
version: '1'

# Shared configuration
shared_config:
  storage: &storage
    psql: &psql_conf_shared
      provider: postgresql
      hostname: 127.0.0.1
      port: 5432
      username: postgres
      password: 123456
      database: postgres
      default_table: shared_docs

  message: &message
    amazon_mq: &amazon_mq_conf_shared
      provider: amazon-rabbitmq
      hostname: ${{ ENV.AWS_MQ_HOSTNAME }}`
      port: 5671
      username: ${{ ENV.AWS_MQ_USERNAME }}
      password: ${{ ENV.AWS_MQ_PASSWORD }}
      tls: True
      virtualhost: /

    rabbitmq: &rabbitmq_conf_shared
      provider: rabbitmq
      hostname: ${{ ENV.RABBIT_MQ_HOSTNAME }}
      port: ${{ ENV.RABBIT_MQ_PORT }}
      username: ${{ ENV.RABBIT_MQ_USERNAME }}
      password: ${{ ENV.RABBIT_MQ_PASSWORD }}
      tls: False
      virtualhost: /


# Toast event tracking system
# It can be backed by Message Queue and Database backed
toast:
  native:
    enabled: True
    path: /tmp/marie/events.json
  rabbitmq:
    <<: *rabbitmq_conf_shared
    enabled : True
  psql:
    <<: *psql_conf_shared
    default_table: event_tracking
    enabled : True

# Document Storage
# The storage service is used to store the data that is being processed
# Storage can be backed by S3 compatible

storage:
  # S3 configuration. Will be used only if value of backend is "s3"
  s3: &s3_conf_shared
    enabled: True
    metadata_only: False # If True, only metadata will be stored in the storage backend
    # api endpoint to connect to. use AWS S3 or any S3 compatible object storage endpoint.
    endpoint_url: ${{ ENV.S3_ENDPOINT_URL }}
    # optional.
    # access key id when using static credentials.
    access_key_id: ${{ ENV.S3_ACCESS_KEY_ID }}
    # optional.
    # secret key when using static credentials.
    secret_access_key: ${{ ENV.S3_SECRET_ACCESS_KEY }}
    # Bucket name in s3
    bucket_name: ${{ ENV.S3_BUCKET_NAME }}
    # optional.
    # Example: "region: us-east-2"
    region: ${{ ENV.S3_REGION }}
    # optional.
    # enable if endpoint is http
    insecure: True
    # optional.
    # enable if you want to use path style requests
    addressing_style: path

  # postgresql configuration. Will be used only if value of backend is "psql"
  psql:
    <<: *psql_conf_shared
    default_table: store_metadata
    enabled: False

# Job Queue scheduler
scheduler:
  psql:
    <<: *psql_conf_shared
    default_table: job_queue
    enabled : True

# FLOW / GATEWAY configuration

with:
  protocol:
    - grpc
#    - http

  discovery: true
  discovery_host: 0.0.0.0
#  discovery_host: mariectl-002:2379,mariectl-003:2379,mariectl-004:2379 # SINGLE HOST OR A LIST OF HOSTS
  discovery_port: 2379
  discovery_watchdog_interval: 2
  discovery_service_name: gateway/marie
  kv_store_kwargs:
    provider: postgresql
    hostname: 127.0.0.1
    port: 5432
    username: postgres
    password: '123456'
    database: postgres
    default_table: kv_store_worker
    max_pool_size: 5
    max_connections: 5

  host: 0.0.0.0

# monitoring done by grafana/prometheus has been deprecated in favor of open-telemetry
#  monitoring: false
#  port_monitoring: 57843
  tracing: False
  traces_exporter_host: http://0.0.0.0
  traces_exporter_port: 4317

  metrics: False
  metrics_exporter_host: http://0.0.0.0
  metrics_exporter_port: 4317

  event_tracking: True
  expose_endpoints:

    /document/status:
      methods: [ "POST" ]
      summary: Check status
      tags:
        - document


prefetch: 1

# With how we are going to deploy the application we need to make sure that Executors get the right configuration for Storage and Toast
# This is because we are launching the process with SPAWN and the configuration is not inherited from the parent process

executors:
   - name: annotator_table_parser
     uses:
       jtype: DocumentAnnotatorTableParserExecutor
       metas:
         py_modules:
           - marie.executor.extract
       with:
         storage:
           # postgresql configuration. Will be used only if value of backend is "psql"
           psql:
             <<: *psql_conf_shared
             default_table: extract_metadata
             enabled: True
           s3:
             <<: *s3_conf_shared
             enabled: True
     timeout_ready: 3000000
     replicas: 1
  #    : ${{ CONTEXT.gpu_device_count }}
     env :
       CUDA_VISIBLE_DEVICES: RR


# Authentication and Authorization configuration
auth:
  keys:
    - name: Default Service-Integration
      api_key: mas_0aPJ9Q9nUO1Ac1vJTfffXEXs9FyGLf9BzfYgZ_RaHm707wmbfHJNPQ
      enabled: True
      roles: [ admin, user ]

    - name: Default Testing-Integration
      api_key: mau_t6qDi1BcL1NkLI8I6iM8z1va0nZP01UQ6LWecpbDz6mbxWgIIIZPfQ
      enabled: True
      roles: [ admin, user ]
