jtype: Flow
version: '1'

# Shared configuration
shared_config:
  storage: &storage
    psql: &psql_conf_shared
      provider: postgresql
      hostname: 172.16.11.163
      port: 5432
      username: postgres
      password: 123456
      database: postgres
      default_table: shared_docs

  message: &message
    amazon_mq :  &amazon_mq_conf_shared
      provider: amazon-rabbitmq
      hostname: ${{ ENV.AWS_MQ_HOSTNAME }}
      port: 5671
      username: ${{ ENV.AWS_MQ_USERNAME }}
      password: ${{ ENV.AWS_MQ_PASSWORD }}
      tls: True
      virtualhost: /


    rabbitmq :  &rabbitmq_conf_shared
      provider: rabbitmq
      hostname: ${{ ENV.RABBIT_MQ_HOSTNAME }}
      port: ${{ ENV.RABBIT_MQ_PORT }}
      username: ${{ ENV.RABBIT_MQ_USERNAME }}
      password: ${{ ENV.RABBIT_MQ_PASSWORD }}
      tls: False
      virtualhost: /


# Toast event tracking system
# It can be backed by Message Queue and Database backed
toast:
  native:
    enabled: True
    path: /tmp/marie/events.json
  rabbitmq:
    <<: *rabbitmq_conf_shared
    enabled : True
  psql:
    <<: *psql_conf_shared
    default_table: event_tracking
    enabled : True

# Document Storage
# The storage service is used to store the data that is being processed
# Storage can be backed by S3 compatible

storage:
  # S3 configuration. Will be used only if value of backend is "s3"
  s3:
    enabled: True
    metadata_only: False # If True, only metadata will be stored in the storage backend
    # api endpoint to connect to. use AWS S3 or any S3 compatible object storage endpoint.
    endpoint_url: ${{ ENV.S3_ENDPOINT_URL }}
    # optional.
    # access key id when using static credentials.
    access_key_id: ${{ ENV.S3_ACCESS_KEY_ID }}
    # optional.
    # secret key when using static credentials.
    secret_access_key: ${{ ENV.S3_SECRET_ACCESS_KEY }}
    # Bucket name in s3
    bucket_name: ${{ ENV.S3_BUCKET_NAME }}
    # optional.
    # Example: "region: us-east-2"
    region: ${{ ENV.S3_REGION }}
    # optional.
    # enable if endpoint is http
    insecure: True
    # optional.
    # enable if you want to use path style requests
    addressing_style: path

  # postgresql configuration. Will be used only if value of backend is "psql"
  psql:
    <<: *psql_conf_shared
    default_table: store_metadata
    enabled : False


# FLOW / GATEWAY configuration

with:
  port:
    - 51000
    - 52000
  protocol:
    - http
    - grpc
  discovery: True
  discovery_host: 172.16.11.162
  discovery_port: 8500

  # monitoring
  monitoring: true
  port_monitoring: 57843

  event_tracking: True

  expose_endpoints:
    /text/extract:
      methods: ["POST"]
      summary: Extract data-POC
      tags:
        - extract
    /status:
      methods: ["POST"]
      summary: Status
      tags:
        - extract

    /text/status:
      methods: ["POST"]
      summary: Extract data
      tags:
        - extract

    /ner/extract:
      methods: ["POST"]
      summary: Extract NER
      tags:
        - ner

executors:
  - name: extract_t
    uses:
      jtype: TextExtractionExecutor
      metas:
        py_modules:
          - marie.executor.text
    timeout_ready: 3000000
    #    replicas: 1
    replicas: ${{ CONTEXT.gpu_device_count }}
    env :
      CUDA_VISIBLE_DEVICES: RR

#  - name: ner_t
#    uses:
#      jtype: NerExtractionExecutor
#      with:
#        model_name_or_path : 'rms/layoutlmv3-large-corr-ner'
#        <<: *psql_conf_shared
#        storage_enabled : False
#      metas:
#        py_modules:
##          - marie_server.executors.ner.mserve_torch
#          - marie.executor.ner
#    timeout_ready: 3000000
##    replicas: 1
#    replicas: ${{ CONTEXT.gpu_device_count }}
#    env :
#      CUDA_VISIBLE_DEVICES: RR
##
#  - name: overlay_t
#    uses:
#      jtype: OverlayExecutor
#      with:
#        model_name_or_path : 'rms/holder'
#        <<: *storage_conf
#        storage_enabled : True
#      metas:
#        py_modules:
#          - marie.executor.overlay
#    timeout_ready: 3000000
#    replicas: 1
#
